- Reason to know classical algorithms: many practical problems map to classical solutions; solved problems often move out of AI; symbolic AI builds on classical foundations.

- Programming/testing requirements:
  - Put imports in their own code cell.
  - Functions must be pure (take inputs, return outputs), no OOP for course assignments.
  - Each function requires three tests (use assertions); test extremes: zero, one, few/many, none/boom.

- Example problems and classical-algorithm mappings:
  - Fix corrupted corporate parent–subsidiary relationships (including loops): Union-Find to identify connected components and clusters.
  - Detect circular dependencies in workflows: depth-first search (cycle detection).
  - Determine build order / dependency resolution: topological sort.
  - Find interdependent script/module clusters (mutual dependencies): strongly connected components (Tarjan’s algorithm).
  - Fast price lookup in a sorted catalog: binary search.
  - Compare text similarity for customer messages: longest common subsequence or edit distance (use embeddings/LMs only if semantic/paraphrase detection required).
  - Maximize number of jobs before deadlines: greedy scheduling algorithms.
  - Fastest delivery route / shortest paths: Dijkstra’s or Bellman-Ford.
  - Resource allocation / sequential decision problems: dynamic programming (originated in AI; useful for reuse of subproblem solutions).
  - Autocomplete: trie/tree structures.
  - Determine operational/market boundaries from points: convex hull.
  - Find repeated substrings / text fingerprinting: suffix trees (plagiarism, repeated phrases).
  - Assign staff to shifts / matching problems: bipartite matching.
  - Fast probabilistic membership checks (caching, duplicate detection): Bloom filters.
  - Best product mix under constraints: knapsack problem (dynamic programming/algorithms).
  - Supply-chain shipping and allocation under cost constraints: linear programming (solvable with SciPy; outputs specific shipment plan and cost).

- When a problem crosses into AI:
  - Need to infer hidden or missing relationships from noisy data, natural language, or learned patterns.
  - Need to predict or adapt under uncertainty (traffic prediction, demand forecasting, execution success probabilities).
  - Need personalization, learning from user feedback, or adapting to dynamic constraints.
  - Need semantic understanding beyond syntactic similarity (paraphrase detection, subject/verb weighting) — embeddings/LLMs may help but have limitations (e.g., semantic weighting issues).

- Practical guidance for stakeholders when AI is not initially required:
  - Start with a classical deterministic solution as a stable baseline and validate logic/requirements.
  - Design system to allow later AI layering for adaptation, personalization, or prediction.
  - Log edge cases for future ML training; use hand-tuned heuristics as interim solutions.
  - Frame logic as constraints to enable later use of constraint solvers or planners.
  - Visualize decision space to show where ML would add value.

- General takeaway: many problems that feel like AI are solvable with classical graph/algorithmic methods; reserve AI when inference, learning, adaptation, or semantic interpretation are truly required.

- Course/module covers high-level AI concepts, history, frameworks, and ethics up front.
- Two broad AI categories: Systems AI (embodied agents perceiving and acting in environments; e.g., robots, game-playing, planning) and Internet AI (models embedded in systems for predictions/classification; e.g., churn, spam, recommendations, fraud detection).
- Norvig & Russell 2x2 framework: think like humans / act like humans / think rationally / act rationally. Acting rationally (rational agents) identified as the standard model.
- Agent abstraction: percept history → agent function → action; external performance measure evaluates behavior.
- Standard model details: rational agent maximizes expected performance using available information/beliefs, objectives represented as utility (loss/error functions), actions and policies; handles uncertainty and trade-offs.
- Two refinements to the standard model:
  - Bounded rationality (satisficing under resource and information constraints).
  - Uncertain human objectives (objectives may be unknown; motivates preference learning, assistance, corrigibility).
- Embodied AI perspective: intelligence situated in the body; rejects strict mind/body dualism.
- Philosophical and mathematical roots: Cartesian dualism, symbol manipulation, internal representations, logic/inference; mathematics and computation underlie algorithms (importance of linear algebra and GPUs for modern GenAI).
- Decision theory/economics: preferences as utilities, beliefs as probabilities, expected utility maximization frames agent behavior and trade-offs (including attitudes toward risk).
- Neuroscience and cognitive psychology provide inspiration but have limits due to biological/computational mismatches; neural network analogies are imperfect.
- Linguistics: language structure (syntax, semantics, pragmatics) is central to representing information and connecting to beliefs/knowledge; language models can be fluent yet produce falsehoods.
- Hardware and software engineering shape feasibility, scale, speed, maintainability, reproducibility, and deployment.
- Control theory informs optimal actions, robotics, and reinforcement learning.
- Historical cycles: eras of symbolic logic, probabilistic models, learning-from-data, repeated hype/AI winters, and integration of past lessons; examples: symbolic AI fragility, early neural network limitations, shifts among models (SVMs, random forests, deep learning).
- Key application domains: vision, language, planning, robotics, recommendation systems, fraud detection, scientific discovery.
- Evaluation beyond accuracy: fairness, safety, calibration, robustness to concept/dataset drift.
- Ethics, risks, and governance concerns: bias, discrimination, privacy, safety, reliability, misuse (e.g., deepfakes); need for transparency, accountability, oversight; lack of professional enforcement analogous to medical oaths.
- Long-term control and alignment: advanced systems can generalize unexpectedly; importance of specifying constraints and objectives to prevent harmful behavior.
- Examples of mitigation/research directions: preference learning, corrigibility, constitutional constraints, designing system prompts and guardrails.
- Practical observation: GenAI is a small subset of AI; models can behave surprisingly well even when underlying reasons are not fully understood.
- High-level unifying lens: rational agents operating under uncertainty and constraints; AI draws on logic, probability, computation, decision theory, and economics.
- Takeaway recommendations: design ethics and control into systems from the start rather than retrofitting.
- Reflection prompts: classify large language models in the think/act × human/rational matrix; give an objective that is easy to state but hard to specify/implement; assess current position in AI progress cycles.

- Definitions and distinctions
  - Weak AI: machines simulate intelligent behavior without claiming consciousness; focus on functional usefulness and acting rationally.
  - Strong AI: claims machines could have genuine minds, consciousness, understanding, self-awareness, intentionality; considered speculative.
  - Free will and consciousness remain unresolved philosophical problems with no operational definitions.

- Turing test and chatbots
  - Turing reframed "Can machines think?" as observable behavior: can an evaluator distinguish a machine from a human in conversation.
  - Historical chatbots (e.g., ELIZA) and modern LLM-based chatbots often pass informal Turing-like tests.
  - Behavioral indistinguishability does not prove internal models, reasoning, or consciousness.
  - Real-world consequences of convincing chatbots: reported 2025 case of an elderly person with dementia who died after following a chatbot’s instructions to meet someone.

- Capabilities and limitations of current models
  - Assertion: current LLMs (e.g., ChatGPT) are not forms of strong AI; they lack proven internal models and true reasoning.
  - Models work well empirically, but mechanisms for why they perform so well are not fully explainable.

- Risks, misuse, and unintended side effects
  - AI as a force multiplier: small inputs can yield essays, propaganda, deepfakes, dangerous instructions.
  - Examples of misuse / failures:
    - Early prompt-engineering bypasses enabling requests for harmful recipes (ground glass, napalm).
    - Dealership in Monterey, CA: website pipeline to ChatGPT produced a $1 car deal.
    - Air Canada: chatbot generated an imaginary refund policy that a court required the airline to honor for that transaction.
    - UK case: person relying on chatbot for psychiatric advice with poor outcome.
    - Code-generation tools (Copilot/Claude) produced code that caused a company to drop its production database.
  - Lethal autonomous weapons highlighted as extreme-risk scenario analogous to automated trading gone wrong.
  - Security and privacy breaches, bad actors, and errors remain concerns.

- Ethical and legal concerns
  - Copyright and training data: models likely trained on copyrighted texts scraped from repositories (e.g., LibGen); debates over consent and legality.
  - Getty Images/public-domain image lawsuit (2024) example showing complexity of copyright enforcement.
  - Worker displacement: white-collar jobs appear vulnerable to automation; societal and economic impacts uncertain.

- Fairness, trust, transparency, and accountability
  - Fairness dimensions: group fairness, individual fairness, equal opportunity; trade-offs exist and cannot all be maximized simultaneously.
  - Evidence-weighting problem: difficulty reconciling many low-quality sources vs. fewer high-quality sources.
  - Trust issues: models are often not trustworthy; users may demand human oversight even when model accuracy is high.
  - Explainability/transparency deficits: unclear why particular outputs are produced; difficulty knowing whether personal data were used or removed from models.
  - Regulatory differences: Europe (GDPR, stronger responsibility-to-reduce-harm stance) vs. weaker/lagging U.S. regulation.
  - Accountability questions: who is responsible when AI gives harmful or illegal advice?

- Safety, oversight, and governance
  - Need for proactive safety, oversight, alignment with societal values, and human-in-the-loop for critical decisions to maintain accountability and mitigate catastrophic errors.
  - Challenges: governance and regulation that balance innovation with ethics and safety; distributional shifts; value alignment across diverse human values.

- Practical/educational responses
  - Academic integrity: class policy forbids use of generative AI for assignments to ensure foundational learning and critical evaluation skills.

- Key takeaways and open questions
  - Distinction between simulating intelligence and genuine minds remains central.
  - Behavior-based tests (Turing) have limits; consciousness remains unsolved.
  - Ethical imperatives: reduce harm, ensure fairness, increase trust and transparency.
  - Generative AI raises new challenges: copyright, displacement, misinformation, safety.
  - Open questions: Can machines have minds or only simulate intelligence? How to operationalize fairness? What concrete steps mitigate generative-AI harms?

- Roadmap: agents and agent function; performance measures; PEAS (Performance, Environment, Actuators, Sensors); environment dimensions; objective uncertainty; agent program designs; learning agents; generative AI; key takeaways.

- Agent: entity that perceives via sensors and acts via actuators (examples: cameras, microphones, tactile sensors, GPS; wheels, hands, steering, fare processing).

- Agent function: abstract mapping from percept histories to actions; can be deterministic (same percept history → same action) or stochastic (same percept history → possibly different actions).

- Performance measure: quantitative metric of success; basis for defining rational action; environment-dependent; can be multi-objective.

- Rational agent: selects actions to maximize expected performance given percept history, knowledge of the environment, and uncertainty handling.

- PEAS framework: specify Performance measure(s), Environment, Actuators, Sensors. Example — taxi:
  - Performance: on-time passenger delivery, customer satisfaction
  - Environment: streets, traffic lights, other cars, passengers
  - Actuators: acceleration, braking, steering, fare processing
  - Sensors: GPS, cameras, LiDAR, passenger requests (natural language)

- Environment dimensions:
  - Fully vs partially observable: full access to environment state vs only partial percepts
  - Single-agent vs multi-agent: other agents may be cooperative or competitive
  - Deterministic vs stochastic: successor state fully determined vs probabilistic
  - Episodic vs sequential: independent percept-action episodes vs actions affecting future percepts
  - Static vs dynamic; discrete vs continuous; known vs unknown

- Example classifications:
  - Chess: fully observable, deterministic, multi-agent (two-player), sequential, discrete
  - Self-driving car: partially observable, stochastic, multi-agent, sequential, continuous, dynamic, partially known

- Objective uncertainty: true performance objectives may be unknown or hard to specify; risk of optimizing wrong objectives; design must account for uncertainty over goals.

- Agent program: implemented version of the agent function; design choices trade off efficiency, flexibility, compactness; separates abstract function from concrete implementation.

- Agent types:
  - Simple reflex agents: act on current percept only, no internal state; suited to fully observable, simple/episodic domains (analogous to many deployed ML classifiers).
  - Model-based reflex agents: maintain internal state (model of the world) and use it plus condition-action rules.
  - Goal-based agents: reason about actions in terms of achieving explicit goals (symbolic or numeric representations).
  - Utility-based agents: assign utilities to states and choose actions to maximize expected utility.
  - Learning agents: contain performance element plus learning component to improve over time; differ from static ML models trained offline (true learning agents adapt on the job or retrain internally).

- Generative/agentic AI: raises the question of how generative models and agentic systems map onto the traditional agent types and PEAS design.

- Key practical points:
  - Agent = perception→action mapping; agent function is abstract, agent program is implementation.
  - Choose agent design based on environment properties and PEAS specification.
  - Rationality depends on the chosen performance measure.
  - Learning improves agents only if built to learn online or update their models; many common ML deployments are not learning agents.
  - Exercise suggestion: pick a task, specify its PEAS, decide the best agent type, and consider how generative AI could extend or complement that agent.