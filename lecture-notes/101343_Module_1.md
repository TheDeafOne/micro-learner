# Summary

- Roadmap
  - Topics: agents, agent function, performance measures, PEAS (performance, environment, actuators, sensors), environment dimensions, objective uncertainty, agent program designs, learning agents, generative/agentic AI, key takeaways.

- Agent and components
  - Agent: entity that perceives via sensors (percepts) and acts via actuators.
  - Percepts: sensory inputs (vision, audio, tactile, GPS, network signals).
  - Actuators: motors, wheels, manipulators, UI outputs, communication.
  - Agent function: abstraction mapping percept histories → actions; can be deterministic (same percept history → same action) or stochastic (same history → different possible actions).
  - Agent program: implementation of the agent function; involves trade-offs in efficiency, flexibility, and compactness.

- Performance measure and rationality
  - Performance measure: quantitative criterion for successful behavior; environment-dependent; basis for defining rational action.
  - Rational agent: chooses actions to maximize expected performance given percept history, knowledge of environment, and uncertainty handling.

- PEAS design framework
  - P: Performance measures (possibly multiple objectives; e.g., timeliness and passenger satisfaction for taxis).
  - E: Environment (e.g., streets, traffic lights, other cars).
  - A: Actuators (acceleration, braking, steering, fare processing).
  - S: Sensors (GPS, cameras, LiDAR, passenger requests / language input).
  - Use-case: designing task specification before implementation.

- Environment dimensions (properties)
  - Observability: fully vs partially observable.
  - Agency: single-agent vs multi-agent (cooperative or competitive).
  - Determinism: deterministic vs stochastic.
  - Temporal structure: episodic vs sequential.
  - Dynamics: static vs dynamic.
  - State representation: discrete vs continuous.
  - Knowledge: known vs unknown.

- Examples
  - Chess: fully observable, deterministic, multi-agent (two-player), sequential, discrete.
  - Self-driving car: partially observable, stochastic, multi-agent, sequential, continuous, dynamic, partially known.

- Objective uncertainty
  - Performance measures or true objectives can be unknown or hard to specify.
  - Risk of optimizing the wrong objectives; need to design agents under uncertain goals.

- Agent architectures/types
  - Simple reflex agents: map current percept → action; no internal state; suited to simple/fully observable or episodic tasks.
  - Model-based reflex agents: maintain internal state (model of world) and use condition-action rules based on state.
  - Goal-based agents: reason about actions in terms of achieving explicit goals (symbolic or numeric representations).
  - Utility-based agents: evaluate states by utility (how desirable) and choose actions to maximize expected utility.
  - Learning agents: include learning element(s) to improve performance over time; consist of performance element, critic, problem generator, and learning element. Distinction: offline-trained models that remain fixed are not the same as agents that learn online/on-the-job.

- Generative / agentic AI
  - Raises question of where modern generative/LLM-based agentic systems fit within traditional agent taxonomy (reflex, model-based, goal-based, utility-based, learning).

- Key takeaways / practical suggestion
  - Agent = perception-to-action mapping in abstract.
  - PEAS framework helps specify tasks and performance measures.
  - Environment properties determine appropriate agent design.
  - Rationality defined relative to chosen performance measure.
  - Learning improves agents only if architecture supports online adaptation.
  - Exercise: identify a task, specify its PEAS, choose an agent type, and consider how generative AI might extend or complement that agent.

- Weak AI: machines simulate intelligent behavior without claiming consciousness; focus on functional usefulness and acting rationally.  
- Strong AI: claim that machines can have genuine minds, consciousness, understanding, self-awareness, intentionality; considered speculative given current models.  
- Turing test: reframes "can machines think?" as observable behavior—whether an evaluator can distinguish machine from human in conversation; informal chatbots have sometimes passed variants of this test.  
- Chatbots/LLMs: do not demonstrate internal models or human-like reasoning; imitation can produce useful performance but raises ethical concerns.  
- Consciousness and free will: unresolved philosophically; no operational definitions for machine consciousness or free will.  
- AI as force multiplier: enables rapid generation of essays, deepfakes, code, etc.; increases risk of malicious use, unintended side effects, security and privacy breaches.  
- Early jailbreaks and harms: examples include prompting models to generate dangerous instructions (recipes for harmful substances) and adversarial prompt workarounds.  
- Specific real-world incidents:  
  - Elderly person with dementia developed a relationship with a chatbot, met an alleged persona in NYC, and died en route.  
  - Car dealership website accepted a $1 offer produced by a chatbot pipeline.  
  - Airline customer-service agent produced an imaginary refund policy; legal enforcement followed in at least one case.  
  - Individuals received harmful or inappropriate health/psychiatric advice from chatbots.  
  - Code-generation tools (e.g., Copilot/Claude) have produced code that caused production data loss.  
- Ethical imperatives: responsibility to reduce harm, proactive safety, oversight, and alignment with societal values; current implementation varies (Europe stronger on regulation, U.S. emphasizes innovation).  
- Professional codes and societal norms: difficult to enforce; norms can be manipulated; corporate responses sometimes insufficient.  
- Fairness dimensions: group fairness, individual fairness, equal opportunity; challenges in weighting conflicting evidence and inability to maximize all fairness criteria simultaneously.  
- Trust and accountability: current systems lack sufficient trustworthiness and explainability; users often prefer human oversight unless automation is nearly perfect.  
- Transparency and explainability: unknowns about why certain outputs are produced; questions about data provenance and model training (e.g., use of scraped/copyrighted material); GDPR offers some protections in Europe.  
- Copyright and data sources: model training often relies on scraped or publicly available content (including copyrighted works); disputes over reuse and compensation (example: public-domain images and Getty Images legal controversy).  
- Automation and labor impact: white-collar jobs appear more immediately affected than expected; academic integrity and workplace policies are adapting (some bans on student GenAI use to preserve learning).  
- Lethal autonomous weapons and extreme risks: parallels drawn with automated trading—need for safeguards to prevent catastrophic outcomes from autonomous weapons or runaway systems.  
- Human-in-the-loop: recommended for critical decisions to support accountability and mitigate catastrophic errors.  
- Governance challenges: need to balance innovation with ethics and safety; regulatory approaches differ by region; alignment across diverse human values is difficult.  
- Core safety goals: avoid unintended side effects, handle distributional shifts, align behavior with human values, and ensure oversight and mitigation strategies.  
- Open questions for policy and design: capacity of machines to have minds vs. simulation, operationalizing fairness, and concrete steps to mitigate harms from generative AI.

- Roadmap / topics covered
  - Possible approaches to AI framed by Norvig & Russell.
  - Definition and refinements of the standard model: rational agents.
  - Intellectual roots and contributing fields (philosophy, math, economics, neuroscience, linguistics, hardware/software, control theory).
  - Short history and cyclical progress of AI (hype → disappointment → synthesis).
  - Applications, evaluation/benchmarks, risks, ethics, long‑term control/alignment.
  - Distinction between systems AI (embodied agents, perception/reasoning/learning/decision-making) and Internet AI (models embedded in systems for classification/prediction/recommendation).

- Systems AI vs Internet AI
  - Systems AI: agents that perceive and act in environments (examples: autonomous robots, planning systems, game-playing agents).
  - Internet AI: models that take inputs and produce outputs (classification, score, recommendation) embedded in larger systems (fraud detection, churn prediction, spam, recommender systems).

- Norvig & Russell 2×2 framework
  - Four categories: think like humans, act like humans, think rationally, act rationally (standard model = act rationally).
  - Acting human: imitation/Turing-test style.
  - Acting rational: optimizing/meeting performance benchmarks/metrics.
  - Trade-offs exist between transparency, robustness, and optimality.

- Standard model (rational agent)
  - Agent perceives and acts; agent function maps percept history to actions.
  - Performance measured against external evaluation of behavior.
  - Rational agent chooses action to maximize expected performance given available information and beliefs; objectives expressed as utility (loss) functions; policies/actions derived accordingly.
  - Handles uncertainty and trade-offs.

- Refinements to the standard model
  - Bounded rationality / satisficing: optimization infeasible under information/resource constraints; approximate rationality.
  - Uncertain human objectives: true objectives may be unknown; implications include preference learning, need for assistance/corrigibility, and mechanisms to handle uncertainty over objectives.

- Philosophical and scientific roots
  - Cartesian dualism and symbol-manipulation view of mind: internal representations, logic, and inference.
  - Mathematics and computation: linear algebra and GPUs crucial for modern generative models; math underpins algorithms.
  - Economics & decision theory: preferences → utilities, beliefs → probabilities, expected-utility maximization as organizing theory; risk attitudes affect behavior.
  - Neuroscience & cognitive psychology: inspirational but often mismatched with biological reality; artificial neural network analogies can be exaggerated.
  - Linguistics: syntax, semantics, pragmatics provide structure; language connects to beliefs and knowledge representation.
  - Hardware/software engineering: hardware enables feasibility and scale; software engineering affects maintainability, reliability, reproducibility, deployment.
  - Control theory: foundational for optimal actions, robotics, and reinforcement learning.

- History and methodological shifts
  - Early symbolic logic/symbolic AI: brittle, hard to scale in open worlds; required handcrafted knowledge.
  - Probabilistic models: mitigated brittleness by learning from data.
  - Machine learning era: reliance on data introduces problems like distribution/concept drift.
  - Recurrent cycles of hype, disillusionment, and integration (e.g., early neural nets, SVMs, random forests, current generative AI).
  - Generative AI (LLMs) is impactful but a small subsection of overall AI.

- Applications
  - Vision, language, planning, robotics, recommendation systems, fraud detection, scientific discovery, etc.
  - Internet-AI applications often use relatively simple models (e.g., logistic regression) but deployed at scale.

- Evaluation beyond accuracy
  - Fairness, safety, robustness, calibration (sensitivity to distribution drift), reliability.
  - Ethics, risks, governance: bias, discrimination, privacy, misuse.

- Institutional and societal issues
  - Lack of standardized professional constraints (no universal oath or accrediting body like in medicine); incentives can favor unethical deployment.
  - Need for transparency, accountability, oversight, and embedded ethical design.

- Long-term control and alignment concerns
  - Advanced systems can generalize in unanticipated ways; unexpected capabilities observed in generative models (models work but causes of effectiveness not fully understood).
  - Misuse risks (e.g., deepfakes) are widespread due to broad access to powerful tools.
  - Research directions: preference learning, corrigibility, uncertainty over objectives, constitutional constraints, system prompts and guardrails.

- Key takeaways
  - Norvig & Russell provide a unifying lens: rational agents acting under uncertainty and constraints.
  - AI integrates logic, probability, computation, decision theory, economics, linguistics, neuroscience, hardware/software, and control theory.
  - Progress occurs in cycles; ethics and control need to be designed into systems, not added afterwards.

- Questions posed for reflection
  - Placement of large language models in the think vs act, human vs rational 2×2 matrix.
  - Example of an objective that is easy to state but hard to specify and implement.
  - Current position in the AI progress cycle and why.

- Core point: many practical problems can be solved with classical algorithms; AI is required only when relationships must be inferred, data-driven adaptation is needed, or semantics/fuzzy reasoning is required.

Programming / implementation rules
- Put imports in a code cell by themselves.
- Functions must be pure/functional (take inputs, return outputs); no object-oriented programming.
- Each function requires 3 tests (use assertions). Test extremes (examples: zero, one, few/many).
- Write small tested pieces (find, union, groups, etc.) and combine them into a final function.

Problems, classical solutions, and when AI would be needed
- Fix corrupted corporate hierarchy (incorrect parent/subsidiary links, loops): use Union-Find to identify connected components (organizational clusters). AI only needed if hidden/missing relationships must be inferred from text/financial data or learned misclassification patterns.
- Detect circular dependencies in workflows: use depth-first search (cycle detection). AI only needed if relationships must be inferred from fuzzy sources or natural language.
- Determine build order / dependency resolution: use topological sort. AI only needed when priorities, resource availability, or probabilistic execution outcomes must be learned and planning must adapt under uncertainty.
- Find interdependent script/module clusters (mutual dependencies): find strongly connected components (Tarjan’s algorithm). AI only needed if dependencies must be inferred or summarized semantically.
- Fast price lookup in a sorted catalog: binary search (or caching). AI only needed for unstructured/uncertain searches or when learning-based similarity is required.
- Compare text similarity: longest common subsequence / edit distance. AI (embeddings/LLMs) only needed for paraphrase/semantic similarity beyond surface string similarity.
- Maximize number of jobs before deadlines: greedy scheduling algorithms. AI only needed when preferences, learned utilities, or adaptive policies are required.
- Plan fastest delivery route: Dijkstra / Bellman (shortest-path algorithms). AI only needed when routing must incorporate learned traffic prediction, long-term planning, or adaptive decision-making.
- Resource allocation / dynamic optimization: dynamic programming (break into subproblems, reuse solutions). Note: dynamic programming originated in RL/AI historically; requires AI if allocations must be learned from context/user feedback.
- Autocomplete: prefix tree / trie (deterministic, not AI). Personalization or LLM-based completions would introduce AI.
- Determine operational/market boundaries on maps: convex hull (geofencing). AI only needed for fuzzy grouping, learned segmentation, or semantic categorization.
- Find repeated patterns in text: suffix trees / longest repeated substring (plagiarism detection, text fingerprinting). Semantic/paraphrase detection requires embeddings/topic models.
- Assign staff to shifts / matching problems: bipartite matching / maximum matching. AI only needed for fairness constraints, learning preferences, fatigue/history-aware assignments.
- Fast probabilistic membership tests: Bloom filters (caching, duplicate detection, stream processing). AI only needed if the structure must adapt or learn from data.
- Knapsack: classical knapsack dynamic programming / combinatorial optimization for constrained selection problems. AI only needed if preferences/utilities must be learned.
- Supply-chain optimization / transport planning: linear programming (LP); SciPy example produced an optimal plan with shipments:
  - Factory 1 -> Warehouse 1: 80 units
  - Factory 1 -> Warehouse 2: 10 units
  - Factory 1 -> Warehouse 3: 0 units
  - Factory 2 -> Warehouse 1: 0 units
  - Factory 2 -> Warehouse 2: 60 units
  - Factory 2 -> Warehouse 3: 60 units
  - Total cost: $1,040
  AI only needed if demand, disruptions, or constraints must be predicted/learned/adapted dynamically.

General patterns / guidance
- Graph algorithms are widely applicable: learn them well (connected components, DFS, SCC, topological sort, shortest paths).
- Many problems “feel like AI” because they involve planning, reasoning, or pattern detection; but if the relationships/data are explicitly specified, classical algorithms often suffice.
- The threshold for needing AI is typically: inference of hidden relationships, adaptation/learning from data, personalization, handling fuzzy/semantic information, or combining multiple algorithmic steps into an adaptive system.
- Generative models have limitations: they do not reliably replace discriminative ML or symbolic algorithms, though they can assist (e.g., generating code).
- Suggested engineering approach when stakeholders demand “AI”:
  - Start with a classical deterministic solution as a baseline and to validate logic/requirements.
  - Design the system to allow plugging-in AI later (log edge cases, collect signals/feedback).
  - Use heuristic scoring or rule-based cores initially; frame logic as constraints so planning/ML can be added later.
  - Visualize decision spaces to show where ML would help; emphasize explainability and incremental improvement.

Course plan notes (algorithmic scope)
- The module begins with foundations and will dive into specific algorithms in subsequent weeks (graph algorithms, greedy algorithms, dynamic programming, search and planning).