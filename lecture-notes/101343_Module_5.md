- Generating more plies (deeper lookahead) improves move selection because it gives a better sense of whether a line leads to a win or loss.
- Exploit symmetry to reduce distinct positions (example: four corner moves in tic-tac-toe are equivalent under rotation), increasing effective ply expansion.
- Use game-tree properties to avoid expanding unnecessary subtrees:
  - Example tree with alternating agent (max) and opponent (min) moves, exploring left/right branches and propagating scores upward.
  - Numerical scenario: one leaf returns 52; other possibilities could be 0 or 100, so exploration continues until a conclusive value (example found 71) is propagated to the min node.
  - Further exploration finds a 93 in a different subtree; min will choose the smaller option (71), so some sibling subtrees can be skipped because their values cannot affect the min choice.
  - Other example values shown: 200, 0, 100, 93, 3, 7 — used to illustrate when subtree exploration is or isn’t necessary because current best/worst choices already determine outcomes.
- Technique name: minimax with alpha–beta pruning.
  - Alpha = highest score seen so far for any max node on the current path.
  - Beta = lowest score seen so far for any min node on the current path.
  - Pruning entire subtrees when their possible values cannot change the eventual choice allows searching several ply deeper.
- Move ordering affects pruning effectiveness; scoring moves to examine the best ones first yields more pruning and deeper search.

- Deterministic games extended to include chance by adding chance nodes to the game tree (model chance as a player).
- Example game structure:
  - Agent has a coin flip outcome state (heads or tails) then chooses Left or Right.
  - Opponent then flips a coin (heads/tails) and makes a Min move.
  - Chance nodes represent the coin flips and can terminate expansion at those points; heuristic evaluation applied at leaf states.
- Min nodes choose the child with the lowest heuristic score (examples given: 10, 6, 12, 16).
- Chance nodes use expected value of heuristic scores:
  - Expected value(Left) = 0.5*10 + 0.5*6 = 8
  - Expected value(Right) = 0.5*12 + 0.5*16 = 14
- Max node selects the action with the highest expected value → choose Right (expected value 14).
- Algorithm name: expectimax — combine expected values at chance nodes with Min/Max at opponent/agent nodes.
- Alpha–beta pruning can be applied to expectimax when scores are bounded.
- Common video-game AI techniques (rather than game-theory search):
  - A* search for pathfinding.
  - Finite state machines for behavior (states: patrol, attack, flee; transitions triggered by events such as seeing an enemy, being defeated, healing).
  - Hand-authored decision trees (not learned from data in typical game AI).
  - Behavior trees.
  - Hierarchical Task Networks (planning).
  - Scripting for explicit NPC behavior.
- Historical resource constraints:
  - Most CPU cycles used for graphics historically; recent GPUs and multicore CPUs free cycles for more advanced AI.
  - Increased use of hierarchical task networks and more complex AI as spare processing becomes available.

- Prisoner's Dilemma example (payoffs = prison years)
  - Two suspects (Peter, John). No communication.
  - If both deny: each gets 2 years.
  - If one confesses and the other denies: confessor gets 1 year, the denied (rat-out) gets 10 years.
  - If both confess: each gets 5 years.
  - Extensive form can be drawn as a sequential tree (used here for illustration), but the interaction is effectively simultaneous; normal form (payoff matrix) also represents the same information.
  - Best-response analysis:
    - If both deny, either can improve by confessing (2 → 1), so deny is not stable.
    - If both confess, neither can improve by unilaterally switching (5 → 10), so (confess, confess) is stable.
  - (Confess, confess) is the Nash equilibrium.
  - Each player’s confess strictly dominates deny (confess yields a strictly lower prison term regardless of the other’s action).
  - Illustrates that individually rational strategies can produce a collectively worse outcome.

- Definitions and notation
  - Players indexed; strategies for player 1: a, b, c; for player 2: x, y, z.
  - Payoff functions f_i,j denote payoffs when player 1 uses a given strategy and player 2 uses a given strategy.
  - Strict dominance: strategy C strictly dominates J for a player if C’s payoff > J’s payoff for every possible opponent strategy.
  - Weak dominance: strategy C weakly dominates J if C’s payoff ≥ J’s payoff for every opponent strategy and > for at least one.

- Examples of dominance comparisons
  - If for each opponent strategy C’s payoff is strictly larger than B’s payoff, then C strictly dominates B (B is dominated).
  - If C’s payoffs are ≥ A’s payoffs for every opponent strategy and strictly greater for at least one, then C weakly dominates A.

- Successive elimination of dominated strategies (iterated deletion)
  - Procedure: identify any dominated strategy for a player and remove it; switch to the other player and repeat, alternating until no dominated strategies remain or a single outcome is reached.
  - Eliminated strategies are removed without regard to opponents’ eliminated strategies; the reduced game is used for the next step.
  - Example (cover choices / profits)
    - Players: The Economist (strategies a, b) and BusinessWeek (strategies x, y).
    - The Economist: strategy b is dominated by a (a yields higher profits than b against each BusinessWeek choice) → eliminate b.
    - With b removed, BusinessWeek: strategy y is dominated by x (x yields higher profits than y given the remaining Economist strategy) → eliminate y.
    - Remaining strategy pair (a, x) is the solution / Nash equilibrium in that reduced game.
  - Notes and caveats:
    - The order of elimination can matter; starting with a different player may produce different intermediate eliminations.
    - Successive elimination does not always yield a unique outcome or fully solve the game; multiple equilibria can exist and the elimination path can determine which equilibrium is found.

- Rock–Paper–Scissors (normal form)
  - Payoffs (for a given player): tie = 0; loss = −1; win = +1.
  - Payoff examples: Rock vs Rock = 0; Rock vs Paper = −1; Rock vs Scissors = +1 (symmetrically for other choices).
  - Zero-sum game: players' payoffs sum to 0.
  - No pure-strategy Nash equilibrium.
  - Mixed-strategy Nash equilibrium: play Rock, Paper, Scissors each with probability 1/3.
  - Expected payoff at equilibrium: 0 for each player.

- Employer–Employee monitoring game (normal form)
  - Actions:
    - Employer: Watch or Ignore.
    - Employee: Work or Shirk.
  - Payoff matrix (Employee payoff / Employer payoff):
    - (Watch, Work): Employee = 50 ; Employer = 90
    - (Watch, Shirk): Employee = 0  ; Employer = −10
    - (Ignore, Work): Employee = 50 ; Employer = 100
    - (Ignore, Shirk): Employee = 100; Employer = −100
  - No pure-strategy Nash equilibrium.
  - Mixed strategies and notation:
    - p = probability employee shirks (so 1 − p works).
    - q = probability employer watches (so 1 − q ignores).
  - Employee expected payoffs:
    - EV(Work) = 50·q + 50·(1−q) = 50
    - EV(Shirk) = 0·q + 100·(1−q) = 100 − 100q
    - Indifference condition EV(Work) = EV(Shirk) ⇒ 50 = 100 − 100q ⇒ q = 0.5
  - Employer expected payoffs:
    - EV(Watch)  = 90·(1−p) + (−10)·p = 90 − 100p
    - EV(Ignore) = 100·(1−p) + (−100)·p = 100 − 200p
    - Indifference condition EV(Watch) = EV(Ignore) ⇒ 90 − 100p = 100 − 200p ⇒ p = 0.1
  - Mixed-strategy Nash equilibrium:
    - Employee shirks with probability p = 0.1 (works with 0.9).
    - Employer watches with probability q = 0.5 (ignores with 0.5).
  - Expected payoffs at equilibrium:
    - Employee: 50
    - Employer: 80
  - Interpretation: each player randomizes daily according to these probabilities (sample from the distribution each day).

- Games discussed: tic-tac-toe, Connect Four, checkers, chess, Go, backgammon, poker.
- Games as state-space search:
  - States: board/game configurations (pieces, cards visible or hidden).
  - Actions: moves, dice rolls, etc.
  - Transition model: given a state and an action, yields the next state and whose turn it is.
  - Costs: typically uniform; payoffs/utilities assigned at terminal states (explicit points or implicit, e.g. +1 win, -1 loss, 0 draw). (Example payoff values mentioned: +1 and -10 in an illustrative game.)
- Game-tree terminology:
  - Game tree: succession of states and actions from start to terminals.
  - Root node: initial state (e.g., empty tic-tac-toe board).
  - Apply: a single action by one player (a single ply).
  - Two-ply: one move in game terminology (an action by each player).
  - Terminal/leaf nodes: winning, losing, or tie states.
  - Extensive-form game: full game tree representation; normal form is an alternative representation.
  - A game is "solved" if the entire game tree (all terminal outcomes) can be generated; only a few small games have been solved.
- Minimax decision procedure:
  - Alternate max (agent) and min (opponent) nodes through the tree.
  - Max nodes choose the action with maximum evaluated value; min nodes choose the action minimizing that value.
  - Tie-breaking between equal-valued moves done randomly.
- Practical limits and heuristics:
  - Full game-tree expansion is infeasible for most interesting games.
  - Use depth-limited search (expand a fixed number of plies) and evaluate frontier states with a heuristic evaluation function.
  - Heuristic evaluation: assigns a score to nonterminal game states indicating how likely they are to lead to a win (example range 0–100, with 100 = win).
  - Game heuristics differ from A* heuristics; admissibility is not required or meaningful here—heuristics need only distinguish promising states from poor ones.
  - Depth-limited minimax uses heuristic values at the search frontier and then propagates min/max values back up to choose the best root action.
- Iterative/online behavior and opponent modeling:
  - After the chosen move and the opponent’s response, regenerate the depth-limited search from the new current state and repeat until the game ends.
  - The agent models the opponent using the same heuristic, but the actual opponent may use a different or stronger evaluation.
  - Increasing the search depth from a later node can change the preferred move compared to the earlier shallower evaluation (deeper lookahead can revise earlier choices).