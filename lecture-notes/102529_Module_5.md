- Supervised learning as the core methodology: models generated by feedback using ground-truth/dataset labels.  
- Use of a cost function/criteria to fit data to models.  
- Most competitive supervised algorithms grounded in optimization, statistics, or function approximation.  
- Naive Bayes implemented from scratch using NumPy and scientific/statistics libraries.  
- Demonstration on an artificially created dataset; examination of classification boundaries for several classifiers (including the Naive Bayes implementation) to understand classifier behavior.  
- Router news text classification using TF-IDF features.  
- Classification of StackOverflow forum posts across 20 different languages.

- Setup
  - Two features: x1 and x2.
  - Four classifiers compared (Gaussian Naive Bayes from scikit-learn, a custom Naive Bayes, linear SVM, RBF SVM).
  - Visualization: program that plots decision boundaries.

- Gaussian Naive Bayes / custom Naive Bayes
  - Builds a 2D probability density function (PDF) per class.
  - PDFs produce circular contours when features have equal variance; become ellipsoidal when feature standard deviations differ.
  - Assumes underlying data is normally distributed and features are independent.
  - PDFs produced by the implementations shown are very similar; computation can be fast.

- Linear SVM
  - Fits hyperplanes (linear boundaries) and divides the feature space into regions.
  - Multi-class behavior realized by combining multiple hyperplanes (one-vs-one style), producing many segmented regions.
  - Can produce substantial misclassification where classes are intermingled in regions that a single hyperplane cannot separate.

- RBF SVM
  - Uses a non-linear kernel, producing wiggly/non-planar decision boundaries instead of hyperplanes.
  - Gamma parameter controls boundary flexibility (higher gamma → more wiggly, more localized decision regions).
  - Tends to make fewer mistakes on the shown toy data because it adapts to the data shape; does not assume a specific distribution.

- Dataset and generalization
  - The demonstrated dataset is a toy, easy-to-separate example and not representative of messy real-world data.
  - Naive Bayes variants can be less general (prone to underfitting if assumptions violated); some classifiers can overfit if too flexible.
  - Preference for classifiers that generalize well; regularization and avoidance of overfitting are crucial in supervised learning.

- Supervised learning
  - Requires known ground truth; algorithm uses feedback from ground truth to test hypotheses and compute errors.
  - Hypotheses are falsifiable conditions; model is built from the set of hypotheses that minimize error / maximize reward.

- Problem types
  - Classification: predict categorical labels (binary or multi-class). Examples: yes/no, low/medium/high.
  - Regression: predict continuous (real-valued) variables; treated as curve fitting; no probability attached in basic regression.

- Multi-class with binary classifiers
  - If base classifier is binary, use one-vs-one scheme: build n choose 2 classifiers (e.g., for 5 classes build 5*4/2 classifiers).
  - Final label via majority voting or aggregating classifier outputs.
  - Common libraries (e.g., SVM implementations) handle one-vs-one training and majority voting automatically.

- Perceptron
  - Early algorithm; building block for neural networks and inspiration for SVMs.
  - Finds an m-dimensional hyperplane separating two classes (binary classifier).
  - Training based on accumulating prediction error across data points.
  - Cost/error expressed as a summation over differences between actual and predicted class labels (0/1).
  - Data notation: n data points (rows), m features/dimensions (columns); hyperplane in m dimensions.

- Naive Bayes
  - Probabilistic classifier based on Bayes' rule.

- Linear regression
  - Predicts continuous outputs by minimizing an error (cost) function; example: linear fit y = b0 + b1 x with b0 offset and b1 slope.
  - Predicted values range over (−∞, +∞); closed-form solutions exist for linear parameter estimation.
  - Contrast with perceptron: regression errors and predicted values are continuous rather than binary (0/1).
  - Logistic regression is mentioned as a classifier based on regression and covered elsewhere.

- Visualization
  - Perceptron visualization demonstrated using interactive widgets.

- Environment and widget
  - Perceptron visualization widget loaded into an Anaconda notebook.
  - Widget initially failed to run due to a missing state; rerunning notebook cells restored it.
  - Visualization implemented as an interactive figure that recomputes classifications when parameters change.

- Data and feature space
  - Two-dimensional feature space with features x1 and x2 (X is a matrix with two columns).
  - Feature ranges set (e.g., x1/x2 minimum = -10, maximum = 10).
  - Grid/mesh of points created with np.linspace (11 points per axis in the example), flattened and classified.

- Hyperplane and parameters
  - Hyperplane defined by w · x + b = 0.
  - w is the normal vector (visualized with red arrows, orthogonal to the hyperplane); b is the offset.
  - Classification: w · x + b > 0 → class 1; w · x + b < 0 → class 0.

- Theta and w construction
  - Single control variable theta (angle in radians) used to set w.
  - w = [cos(theta), sin(theta)] → unit-length normal that rotates as theta changes.
  - Changing theta and b repositions the hyperplane and triggers automatic reclassification of all grid points.

- Implementation details
  - Function to compute hyperplane: given data points, w and b, solve for the other feature (e.g., compute x2 for given x1 from w · x + b = 0) to plot the line.
  - Perceptron classifier implemented as f(x) = sign(w · x + b), returning 1 if dot + b > 0 else 0.
  - Uses numpy dot product; implementation generalizes to n dimensions without changing the perceptron computation.
  - Classification applied to all mesh grid points; negative-class points collected into one list (P1) and drawn with '0' marker, positive-class points into another (P2) and drawn with '+' marker.

- Notes on extensibility and context
  - Two-dimensional plotting solves for one coordinate given the other; in 3D an analogous approach would solve for the third coordinate given two.
  - Perceptron is a linear classifier; combinations of such linear units form building blocks of neural networks. Support Vector Machines use kernels to produce non-linear decision boundaries.

- Classifier and theory
  - Naive Bayes using Bayes' rule: posterior ∝ likelihood × prior.
  - Independence assumption: features treated as independent; feature likelihoods multiplied across features.
  - Numerical inputs modeled with Gaussian PDFs (norm.pdf); categorical inputs can be handled via one-hot encoding or a custom nominal approach.

- Dataset
  - Generated with make_blobs: 5 classes, 2 features.
  - Classes have different centroids; some overlap, some well-separated.
  - Cluster standard deviations vary (examples: 2 for tighter clusters, 5 for more spread).
  - Class sample counts are unbalanced (example counts given: 100, 500, 500, 500, 1000).
  - Random baseline accuracy = 20%.

- Implementation details (custom Gaussian Naive Bayes)
  - API matches scikit-learn style: fit and predict methods.
  - Internal variables: N (samples), M (features), C (number of classes), prior, predictors, likelihoods.
  - Input validation: class labels constrained to integers 0..C-1 (mapping recommended if not).
  - Prior: computed as bincount(y_train) / N (class prior PMF).
  - Predictors: for each class and each feature, fit a Gaussian PDF (mean and std) from training data.
  - Likelihoods: stored per class as arrays of per-feature PDFs.
  - Prediction: for each test sample and each class compute product of per-feature PDFs × class prior; choose class with maximum posterior (argmax).
  - Numerical stability/performance: operations implemented as array operations; custom implementation slower than optimized library.

- Testing and evaluation
  - Reclassification (train and test on same data): ~97% accuracy.
  - 10-fold stratified cross-validation used because classes are unbalanced.
  - Cross-validation results: custom Gaussian NB ≈ 97%; scikit-learn GaussianNB ≈ 97%; linear SVM lower; RBF SVM ≈ 96%.
  - Conclusion from results: Naive Bayes achieved high accuracy (~97%) on this 5-class problem despite simplicity; SVM variants generalize well but gave slightly different performance.

- Practical notes
  - If feature independence assumption is violated (feature correlation), Naive Bayes performance may degrade.
  - Converting between numerical and nominal representations is possible (binning, one-hot); different classifiers handle types differently.
  - Debugging recommendation: run implementation in an IDE debugger (e.g., PyCharm) to inspect intermediate arrays and probabilities.

- Example 1 — Reuters (NLTK corpus) text classification
  - Dataset: NLTK Reuters corpus, ~10,000 documents, originally 90 categories; documents are short (title + a few sentences).
  - Label reduction: collapse into top 5 categories + "other" → 6 classes; map textual labels to numeric indices.
  - Feature extraction: TF–IDF vectorizer from libraries (advanced vectorizer that removes stop words and does preprocessing). Vocabulary ~30,000 unique words → X matrix ~10,000 × 30,000 stored as a sparse matrix.
  - Modeling pipeline: Count/TF–IDF vectorizer + classifier combined in a single pipeline; k-fold cross-validation used for evaluation.
  - Classifiers compared: Linear SVC (LinearSVC), SVC (RBF/kernel), Naive Bayes, Random Forest (parallelized on 8 cores), Logistic Regression, (and mention of neural networks as an option).
  - Performance & runtime notes:
    - LinearSVC produced the best results (≈94% with 10-fold CV) and was very fast (~10–13 s including feature extraction).
    - Logistic regression was slower but accurate (~93% reported).
    - Random Forest ~83%, Naive Bayes ~80%.
    - RBF SVC (kernel computation) was slow (minutes) and not parallelized in that library.
  - Practical points: TF–IDF yields very high-dimensional sparse features; linear classifiers often excel in this regime; LinearSVC is optimized and faster than using SVC(kernel="linear").

- Example 2 — Programming-language detection from StackOverflow posts
  - Dataset: StackOverflow posts, balanced subset with 20 categories, ~40,000 posts (≈2,000 per category).
  - Features: TF–IDF vectorizer with huge vocabulary (~180,000 unique words) → X matrix ~40,000 × 180,000 (sparse).
  - Evaluation: used a 70/30 train/test split (random_state=42 for reproducibility) instead of k-fold due to scale and speed concerns.
  - Results:
    - Overall accuracy ~80.1% (vs random baseline ≈5%).
    - LinearSVC was the fastest and highest-performing among tested classifiers.
    - Logistic regression was much slower for the split evaluation (≈16 minutes).
    - Per-category performance varied: .NET, HTML, iOS performed poorly; Ruby on Rails and Android achieved ~95%.
  - Technical constraints: converting huge sparse TF–IDF matrices to dense can cause out-of-memory errors; cross-validation on this size can be prohibitively slow.
  - Annotation note: labels require human annotation/cleaning; high-quality annotated datasets are costly but critical.

- TF–IDF and modeling insights
  - TF–IDF treats each word as a feature (very high dimensional). IDF downweights ubiquitous words (stop words), producing more meaningful term weights.
  - Sparse storage of TF–IDF matrices is essential for memory efficiency.
  - When number of features ≫ number of samples and TF–IDF is sparse/localized, linear hyperplane classifiers (LinearSVC) perform well.
  - If dense lower-dimensional embeddings are used (word embeddings, PCA-like reduction to 5k–10k features), RBF or kernel SVMs may become more effective because the feature dimensionality is reduced and data becomes denser.
  - Practical model-selection order recommendation: try Naive Bayes, Random Forest, Logistic Regression, SVMs, and neural networks; consider computational cost and parallelization.