- Emphasize objective evaluation of machine learning performance to avoid biased perceptions and deployment failures.  
- Study how to evaluate a learning model's performance.  
- List popular/common ML metrics and explain their context-specific uses.  
- Discuss generalization as the most important property of a learning model and explain why.  
- Identify which classifier algorithms tend to generalize well.  
- Present the receiver operating characteristic (ROC) as a classical evaluation method and demonstrate how to generate it within a learning pipeline.  
- Explain model selection based on ROC operating points by optimizing true positive rate versus false positive rate.

- Model evaluation goal: ensure model generalizes to real-world data and predict deployed performance.

- Data splits:
  - Training: fit model parameters.
  - Testing: check error of trained model.
  - Validation: tune hyperparameters (mutually exclusive; all three together cover the dataset).
  - Wrong practice: repeatedly using train/test to pick hyperparameters (leads to biased, non-general models).

- Hyperparameter examples:
  - Random forest: tree depth can be set ad hoc (possibly skip validation).
  - Support Vector Machine (RBF kernel): Gamma controls decision boundary wiggliness and requires validation to tune.

- Simplest baseline classifier: always predict a single class (e.g., predict 0 every time).
  - Performance equals class prevalence (e.g., 20% for balanced 5-class, 50% for balanced binary).
  - Extremely low accuracy on balanced data indicates a bug.

- Key concepts: generalization and false alarm (false positive) vs detection (true positive).
  - Maximizing detection often increases false alarms; need a compromise (visualized by ROC curve).
  - Example: cybersecurity model that flags every file as malicious achieves 100% detection but 100% false alarm.

- Evaluation approaches:
  - Reclassification: train and test on all data (expect high performance; 100% implies overfitting).
  - k-fold cross-validation: shuffle, split into k non-overlapping folds (e.g., 10-fold: 10% test each round); report average accuracy.
  - Leave-one-out CV for very small datasets.
  - Stratified CV for imbalanced classes so each fold preserves class proportions.

- Model classes with generally good generalization: SVMs, ensemble methods (random forest), PCA for dimensionality reduction, deep neural networks with dropout and L2 regularization, and humans.

- Metrics and math:
  - Accuracy = correct predictions / total predictions.
  - Confusion matrix: rows/columns for ground truth and predictions; can accumulate across CV folds.
  - True Positive (TP), True Negative (TN), False Positive (FP), False Negative (FN).
  - True Positive Rate (recall / sensitivity) = TP / (TP + FN).
  - True Negative Rate (specificity) = TN / (TN + FP).
  - Precision = TP / (TP + FP).
  - F1-score combines precision and recall; preferred over accuracy for imbalanced or when positive class is more important (e.g., cancer detection).
  - Accuracy treats classes equally and may undervalue costly Type I errors.
  - Type I vs Type II errors (historical wording): missing a positive (e.g., undetected cancer) is highest cost; false alarms cause unnecessary costs but are less severe in that context.
  - ROC curve and Area Under Curve (AUC) illustrate trade-off between true positive and false positive rates.
  - Regression metrics: Mean Absolute Error (MAE), Logarithmic Loss (log-loss) for probabilistic predictions.

- Historical context: early detection/classification used probability of detection and false alarm rate; receiver operating characteristic (ROC) developed to trade off detection vs. false alarms.
- ROC curve: y-axis = true positive rate (probability of detection); x-axis = false positive rate (probability of false alarm). Diagonal 0,0â€“1,1 represents random/coin-flip classifier baseline.
- Operating point determined by a threshold on a score/signal; moving the threshold moves along the ROC curve.
- Perceptron analogy: threshold is a decision boundary (a plane in higher dimensions; a single value in 1D).
- Model hyperparameters act as operating knobs that produce ROC operating points (examples: SVM gamma, random forest tree depth/min samples, logistic regression threshold, deep learning batch size/iterations).
- Procedure illustrated using the scikit-learn breast cancer dataset:
  - Features X = all columns except the final "cancer" column; label Y = malignant vs. benign.
  - No explicit train/test split used for the ROC example (entire dataset used to produce scores).
  - Features scaled with StandardScaler to normalize values (typical before logistic regression/SVM).
- Logistic regression setup and hyperparameters:
  - Vary regularization parameter C across values ~0.2 to 100 to generate multiple operating points (nine shown).
  - penalty = 'l1', class_weight = 'balanced' to compensate class imbalance, multi_class='auto', max_iter = 10000, solver chosen by penalty.
  - Each C produces a classifier; scores produce TPR and FPR for that operating point.
- Plotting: operating points computed, sorted and plotted on ROC with annotations (fixed random state for reproducibility).
- Decision trade-offs:
  - For cancer detection, favor high probability of detection even with high false alarm rate (example choice: operating point with ~35% FPR) to catch as many cancers as possible.
  - For nuclear-attack detection, favor very low false alarm rate to avoid catastrophic false positives.
  - For cybersecurity, choose a middle ground to balance analyst workload from false alarms and missed detections.
  - Final operating point selection is a decision-maker trade-off; model developer provides capability/behavior information.
- Theoretical note: true underlying joint probability distribution of the data is unknown; machine learning models estimate that distribution. If the true distribution were known, ML would be unnecessary.