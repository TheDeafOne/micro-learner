- Supervised learning framework
  - Data split into features (x) and labels/targets (y).
  - Train a model f(x) on training set of size n to predict labels for future/unseen instances.

- Statistical inference assumption
  - Future/live data must come from the same distribution as training data (training distribution must represent intended use).
  - Models trained on a limited variety (e.g., only white swans) cannot predict unseen varieties (e.g., black swans).

- Regression metric
  - Common metric: mean squared error (MSE); other metrics possible.

- Classification (binary) setup
  - Four outcomes: true positive (TP), true negative (TN), false positive (FP), false negative (FN).
  - Confusion matrix: rows/columns can be summed to get predicted vs actual class counts; total examples = TP+TN+FP+FN.

- Accuracy and error
  - Accuracy = (TP + TN) / total.
  - Error rate = (FP + FN) / total.

- Class imbalance and unequal error costs
  - Accuracy can be misleading when class distributions are skewed (e.g., majority class high accuracy by naive prediction).
  - FP and FN may have different real-world consequences.

- Precision and recall
  - Recall (sensitivity) = TP / (TP + FN) — proportion of actual positives correctly identified; denominator = total actual positives.
  - Precision = TP / (TP + FP) — proportion of predicted positives that are correct.
  - Precision measures discrimination between classes; recall measures ability to find positives.
  - Precision–recall trade-off commonly encountered.

- Examples of asymmetric costs
  - Edible vs poisonous mushroom: FP (labeling poisonous as edible) is worse than FN.
  - Cancer precursor test: FN (missing a precursor) is worse than FP.
  - These correspond to statistical type I and type II errors (relation between FP/FN and type I/type II).

- Good old-fashioned AI (GOFAI)
  - Focus: problem-solving, general-purpose reasoning
  - Outputs: operational knowledge (plans, how-to)
  - Inputs: often engineered into the reasoner

- Modern machine learning
  - Focus: pattern discovery, data-driven
  - Outputs: representational (models that represent patterns)
  - Requires collections of data to learn from

- Machine learning categories
  - Supervised learning: known features and known outputs (labels)
  - Unsupervised learning: known features only (no known outputs)
  - Reinforcement learning: a third distinct category

- Supervised learning subtypes
  - Regression: output is numeric (example: predict miles per gallon from car weight)
  - Classification: output is a class label (example: predict manufacturer—Japan/Europe/USA—using weight and mpg)

- Example data scenarios
  - Regression example: feature = car weight (pounds), output = mpg (numeric)
  - Classification example: features = weight and mpg; plotted points colored by manufacturer class

- Common algorithms
  - Regression: linear regression, artificial neural networks, fuzzy-rule systems, regression trees, k-nearest neighbor (k-NN)
  - Classification: logistic regression, support vector machines (SVMs), decision trees, Bayesian networks, k-NN
  - Many algorithms have variants applicable to both regression and classification (e.g., SVMs for regression)

- Feature types and representation
  - Types: measurements, counts, rankings, dates, categories
  - Continuous vs discrete: measurements are typically continuous; counts, rankings, dates, categories are discrete but often encoded numerically
  - Algorithm suitability varies by feature type (e.g., k-NN relies on a distance metric and does not natively handle categorical variables)
  - Transformations/encodings can convert feature types but must be chosen carefully to avoid misleading the model

- Model selection
  - No single best algorithm for all problems; each has strengths and weaknesses
  - Typical approach: try multiple algorithms and select the best-performing model for the specific problem

- Linear regression model form:
  - ŷ = θ0 + θ1 x1 + θ2 x2 + θ3 x3 + ...
- Feature scale issue:
  - Features with very different ranges cause disproportionate effects on error and learning (example ranges given: x1 ≈ -8.792 to +3.52; x2 ≈ 3,245 to 8,765; x3 ≈ -0.0062 to -0.00032).
  - Rescale features to comparable magnitudes to avoid this problem.
- Mean normalization (rescaling) formula:
  - x'i = (xi − μi) / (max(xi) − min(xi))
  - Must save these transformation parameters (μi, max, min) to apply the same preprocessing when making predictions.
  - If Y is normalized, model outputs must be denormalized before interpreting predictions.
  - Some algorithms require normalization as part of their proper functioning.
- Categorical variables handling:
  - Binary categorical example (gender): encode as 0/1 (e.g., male = 0, female = 1).
    - This produces different intercepts for the two groups: when variable = 0, intercept = θ0; when = 1, intercept = θ0 + θ1.
    - Binary/dummy/indicator variables shift the intercept but do not change the slope of continuous predictors.
  - Multiclass categorical example (neighborhood type: urban, suburban, rural):
    - Do not encode as ordinal numbers (e.g., 0,1,2) because numerical ordering/implied distances are meaningless.
    - Use n − 1 dummy variables for an n-level categorical variable (e.g., suburban? and rural?; urban implied when both are 0).
    - Adds preprocessing overhead but allows inclusion of categorical predictors.

- Problem setup
  - Goal: model relationship between car weight (feature, in pounds) and miles per gallon (MPG, output).
  - Data (4 observations): (2774, 18), (3329, 17), (3250, 16), (4341, 15).

- Model
  - Linear model: y = m x + b (slope m, intercept b).
  - Compact vector form: ŷ = θᵀx, with implicit x₀ = 1 to absorb the intercept into θ.

- Error / cost function
  - Squared error for one observation: (ŷ - y)² (visualized as area of a box between point and line).
  - Mean squared error (MSE): J(θ) = (1/n) Σ (ŷᵢ - yᵢ)² (sometimes written with a 1/2n factor for calculus convenience).
  - For a single-parameter (slope-only) case, plotting J versus m yields a parabola (quadratic, well-behaved / convex).

- Gradient (partial derivative)
  - Partial derivative of cost w.r.t. θ_j: (1/n) Σ (ŷᵢ - yᵢ) · x_{i,j} (result from chain rule).

- Parameter update (gradient descent)
  - Update rule: θ_j ← θ_j − α · (∂J/∂θ_j), applied iteratively for all j (and summed over observations when computing the derivative).
  - α is the learning rate: a small fraction of the gradient applied each step.
  - Effects of α:
    - Too large → divergence (jumping out of the bowl), failure to converge.
    - Too small → very slow convergence.
    - Choose α by monitoring error reduction and decreasing α if errors increase.

- Convergence and algorithm notes
  - For linear regression MSE, the error surface is convex quadratic → single global minimum, no local minima; convergence to global minimum is guaranteed if α is chosen appropriately.
  - Gradient descent is a general iterative algorithm with many variants; this is the simplest version.
  - Statistics offers an exact analytical (closed-form) solution for linear regression; gradient descent is useful for teaching the minimization concept and necessary when datasets are too large for the analytical solution.

- Cross-validation
  - Shuffle dataset and split into k equal folds (example: k = 5).
  - For each fold i: use fold i as test set and remaining folds as training set, train a model, compute a metric on the test fold.
  - Repeat for all k folds → obtain k metrics; report mean and standard deviation.
  - k can be chosen (e.g., 3, 5, 10) based on dataset size and feature domains; larger feature domains generally need more data.
  - Using training data as the test set is invalid; cross-validation provides held-out evaluation without needing new data.

- Bias–variance trade-off, underfitting vs overfitting
  - Bias: model limitation causes systematic error; model dominates the data (underfitting).
  - Variance: model fits training data closely so predictions vary a lot with data changes (overfitting).
  - Underfitting (high bias): both training and test errors are high and similar.
  - Overfitting (high variance): training error is low, test error is much higher (large gap).

- Diagnosing bias vs variance with learning curves
  - Procedure: repeatedly train on increasing fractions of the training set (e.g., 5%, 10%, 15%, ... up to 100%) and plot training error and test error vs number of training instances.
  - Interpretation:
    - If training and test error converge and remain high → high bias (more data unlikely to help; try more features/feature transforms or more expressive model).
    - If training error is low but test error is much higher and gap persists → high variance (more data or regularization/feature reduction may help).
  - Practical notes: use increments appropriate to dataset size (larger increments for huge datasets); repeat across different fold splits and average to reduce randomness/noise.

- Parameter selection (model hyperparameters)
  - For each candidate parameter value (e.g., polynomial degree, classification threshold), train using the training data and compute training and test metrics.
  - Plot metric vs parameter value: training error typically decreases as model capacity increases; test error typically has an inflection/U-shape.
  - Choose parameter at the test-error inflection/sweet spot balancing under- and overfitting.
  - Examples: polynomial degree for regression, threshold for logistic classification (threshold can be adjusted based on false positive vs false negative costs).

- Additional details and caveats
  - Metrics need not be error rate; use whatever metric is appropriate (e.g., mean squared error for regression).
  - Real-world curves are often noisy; rotating folds and averaging multiple runs improves reliability.
  - Adding more data is not always beneficial—benefit depends on whether the problem is bias- or variance-dominated.

- Problem setup
  - Binary classification encoded as y ∈ {0, 1} (example: predict "made in USA" = 1 from miles-per-gallon).
  - Using ordinary (linear) regression to predict 0/1 is possible but has problems: predictions can lie outside [0,1] and the solution is sensitive to added/outlier points.

- Logistic (sigmoid) model
  - Linear score z = θ^T x.
  - Predicted value (probability) ŷ = 1 / (1 + e^{-z}) ∈ (0,1).
  - Interpret ŷ as P(y = 1 | x).
  - Final class decision typically by threshold: predict 1 if ŷ ≥ 0.5 (threshold can be changed).

- Cost / loss function
  - Use log-loss instead of squared error.
  - Per-example costs:
    - If y = 1: cost = −log(ŷ) (→ 0 when ŷ → 1, → ∞ when ŷ → 0).
    - If y = 0: cost = −log(1 − ŷ) (→ 0 when ŷ → 0, → ∞ when ŷ → 1).
  - Combined per-example loss: −[y log(ŷ) + (1 − y) log(1 − ŷ)].
  - Overall objective J(θ) = (1/n) Σ examples of the combined loss (average cross-entropy).

- Optimization
  - Gradient of J w.r.t. θ_j: (1/n) Σ (ŷ − y) x_j (note: ŷ is the sigmoid of θ^T x).
  - Use gradient descent (or similar) to fit θ.

- Practical notes
  - ŷ will never be exactly 0 or 1, avoiding literal infinite log costs in practice.
  - Logistic regression gives probabilities and is more robust to outliers than straight linear-thresholding.
  
- Multiclass extension
  - One-vs-all (one-vs-rest): train K binary classifiers, each discriminating one class versus the rest.
  - For prediction, compute probabilities from each classifier and choose the class with the highest probability.
  - Literature often finds multiple binary classifiers competitive with single multi-class classifiers; this is a consideration for model selection.

- Discriminant: a separator between classes (in-class vs out-of-class). Linear discriminants can be drawn as straight lines (or hyperplanes).

- Cost functions:
  - Different cost choices produce different discriminants. The log cost (used with logistic) was discussed previously.
  - Hinge loss: piecewise linear "hinge"-shaped loss that yields zero loss inside a margin and increasing loss outside it. Standard form (for labels y ∈ {+1,−1}) is L = max(0, 1 − y·f(x)).
  - Generic cost can be written as C times a sum combining contributions for positive and negative labels (analogous in structure to the logistic cost but with a different per-example cost function).

- Support Vector Machine (SVM):
  - Uses hinge loss to produce a wide-margin classifier that maximizes the margin between classes.
  - Support vectors: training points that lie on the margin boundaries; they determine the final discriminant.
  - Soft-margin SVM: introduces parameter C to trade off margin width vs. fitting training points (handles non-separable data). C controls how outliers/misclassifications are treated; smaller C increases tolerance to errors (wider margin), larger C penalizes errors more (narrower margin).
  - Typical solution method: quadratic programming (not gradient descent).

- Linear vs. non-linear separability:
  - A concept is linearly separable if a hyperplane can perfectly separate the classes; otherwise it is non-linearly separable.
  - Examples: some 1D/2D class arrangements are separable by a point/line, others are not.

- Kernel trick and feature projections:
  - Mapping inputs to a higher-dimensional feature space φ(x) can make a non-linearly separable problem linearly separable (example: augment x with x^2).
  - Kernel function computes dot-products in feature space without explicit projection. Embedding a kernel in an SVM is the kernel trick.
  - Common kernels: polynomial, Gaussian (RBF), etc. The kernel trick enables SVMs to be powerful beyond simple linear discriminants.