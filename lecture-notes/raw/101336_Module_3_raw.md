None

Hello, I'm Dr. Mccullough.
This is a short lecture on
legal issues associated with
web scraping and
social media data collection.
What do we have to be aware
of as researchers collecting
data If you are
working for a social media platform,
what considerations do you
have to be aware of as well?
The first issue is
this idea of online trespass.
A platform has a platform
or a site that they feel that they
own and other members of the public that have
no affiliation might be
able to go to that site.
Is that like walking into your house, right?
In a case. Facebook for example, right?
So Facebook has this platform.
And if I go into the platform,
right, as a persona and I
have access to data there,
is it like I'm in your house?
And if I do that without agreeing
to your terms of use
and without your permission,
is it like I'm trespassing,
like I've broken into your house?
Well, web scraping in
general went unchal***ged until about 2000.
And then what happened is Bidders Edge
was collecting all this data off of Ebay.
Ebay chal***ged bidders edge on this concept
of trespass and they settled out of court.
There's actually no ruling at that time.
The judge held an injunction
stating that heavy
bought traffic from bidders
Edge could disrupt service.
That would be a denial of service.
So they didn't hold that trespass
was equiva***t in this online environment.
What they did say though,
is if you are
a person that is going to a platform that
has a legitimate business
and your activities on
that platform are disrupting
their ability to execute their business,
then that's wrong and then
you can be liable for that.
Bidders Edge wasn't wrong for
going and collecting data off of ebay.
Whether ebay wanted them to or not,
what they were wrong for doing is having
so much traffic that it actually
affected the performance of the system.
There have been several other cases
on trespass that were also unsuccessful.
So perfect ten versus Google,
Event versus event Bright.
And you can always Google these and get
a far more detail depending on
how much depth you want to go in these.
But the main point of this topic and
these cases are that the idea of trespass,
the idea that a social
media platform owns the house,
so to speak, and your presence there,
collecting data, is
trespass is completely unsupported.
It's more like if the public can get to it,
it's like the sidewalk
in front of your house, right?
The sidewalk in front of your house, yes.
It's kind of on your property.
Can't go and prevent people from walking
along the sidewalk on your house.
I suppose you could block
the car in the driveway
so that it makes it annoying for people,
but it's not like you
can tell them that they're not allowed
to be there or take legal action because
they're walking along
the sidewalk in front of your house.
The next set of issues
came a number of years later,
and these were associated around copyright.
Facebook sued Power.com saying that
scraping the data was a copyright violation.
Another one, the Associated Press
versus Meltwater sued for copyright,
for scraping and publishing
their articles as is.
Using facts or using
information you've scraped is
not a copyright violation.
That's taking information that's
fair use and that's acceptable.
But if you're going to take
the content as is as a way of circumventing
the paper use of
that site content that
would be considered wrong.
Copyright is a little bit more
difficult in terms of the issue,
but it's generally
understanding copyright laws
is pretty analogous, right?
There are a few other interesting ways
of getting around copyright.
Get pocket is a site,
they advertise themselves as
a digital bookmarking
or content aggregation service.
I have a friend
that he's a Christian missionary.
He's doing a platform to help people take
their next best steps in following Jesus.
What he wants to do is
direct people to other content,
Bible studies, platforms, et
cetera to help connect them.
He is considering using Get
Pocket as a way
of sourcing Christian content,
offering it up to people to choose.
But what they're doing there is
he's not taking the
content and re posting it.
What they're doing is they
are bookmarking the content to
direct users to the original sites
that has not been chal***ged yet.
That's generally acceptable if
you're citing the source,
you're giving credit,
and you're directing people,
and you're not diverting them from
the original issue, from the original intent.
It appears that doesn't
become a copyright violation.
Google has a different approach.
They do accelerated mobile pages.
What they're doing is they're encouraging
the original author to host
their content in a way
that makes their content
more accessible and user friendly.
These are ways in which
people are beginning to think about
how they get around
the copyright issues to be
able to offer additional value
added to that material,
whether it's getting, finding it,
getting access,
studying it, investigating it.
But generally with copyright,
if you can use it as fair use,
if you're using
the information derived from it,
the raw data from it,
and you're not copyrighting the image
or pretending like
it's your original content.
The copyright issues don't
become a big issue.
There is something called
the Digital Mil***nium Copyright Act.
This law was enacted
in the late '90s and it was
addressing various issues with
digital media and online content.
I just want to take a few of
these issues and just
explore them a little bit.
The online copyright
infringement liability limitation,
or Safe Harbor clause in this act
shields the online providers
from the activities of their users.
That means that if you take
copyrighted material,
you steal somebody's logo ideas,
paper, and you present it as
your own blatant
copyright violation on Facebook.
Facebook, it's not liable for
your action of copyright
violation on their platform.
Right. That shields the online providers.
That's one of the ways in which Get Pocket is
protected in being able
to bookmark and direct people.
If you're a user on Get pocket
and you host copyrighted information,
then get pocket is
not liable for your action.
There is a provision for
notice and takedown procedures,
so that means a copyright holder can
request to the platform
the removal of infringing content.
You see this a lot with videos, right,
Where somebody will go and tell Youtube,
hey, that is a violation of copyright, right?
We saw this with
the Nestle Greenpeace example where
Green Piece was making a parody
of the kit cat given a break commercial.
And instead of calling it a kit cat,
they called it a killer bar.
And the logo of the killer bar
was very similar to Kcat.
Nestle goes to Youtube and says, hey,
based on the provision of this act,
you have to take that down now.
Youtube has an obligation
to keep it down, right?
So if you look a little bit
further, that notice and stay down,
the platform is responsible
for preventing the re upload of any content.
When I was working at Accenture,
we had a project with Youtube where we would
do basically streaming video recognition.
If I take down
your content and then change the coloring,
you tweak it a little bit, you re edit,
and then you upload that content
back onto the site.
The AI will automatically
detect it and prevent you from uploading it.
And then you actually have to ask
somebody to look at
it to evaluate whether they can repost it.
That's part of the notice
and stay down requirement.
The platforms have to abide by.
There's also anti circumvention provisions,
and this prevents circumvention
of digital rights management.
So like if you have measures to
control access to copyright content,
and you work around that to
get copyright copyright content,
that's not allowed under this act.
It doesn't say that you can't circumvent
digital rights management to just
access information or raw data.
It says you can't do
it for copyright content,
and I think the case law supports that.
That's what the idea is,
is stealing the ideas
of people, not the data.
And then there's also
some exceptions
for distance education and online learning.
So if you're in an online learning class
and see people are
posting a paper or whatever,
there's some limitations and exceptions on
some of these rules to
make that a little easier.
Another issue that comes up
is this idea of fraud and abuse.
Again, this is very
related to the Facebook suing Power.com
saying that scraping
was a copyright violation
and that it was abusing their data.
Linked in, sued to
data analytics company Blabs,
scraped publicly available data or
user data from link it in website linked in,
argued that this was
violating the Computer Fraud and Abuse Act.
Now keep in mind they couldn't
use trespass because
that case law was pretty clear.
So they attempted to use
the Fraud and Abuse Act.
Labs simply contended that
the data was publicly accessible.
They didn't have to get a password protect,
the data wasn't password protected.
They didn't have to log on with a password to
get privileged access to
the site to be able to
get access to the data.
Therefore,
they argued that anything that they
could get from
the open internet was fair game,
And the Ninth Circuit Court of Appeals
ruled in favor of K Labs.
Allowed. Not only did
linked in not have a case,
Q Labs was able to continue
scraping public data linked in.
Of course what is linked in do,
they're going to change their API.
They're going to change
their platform in some way
to protect their data
because of the value, right?
So that's another reason
why API's sometimes change.
Let's look a little bit more at what
the Fraud and Abuse Act really says.
It's a United States law.
It was also in the '80s that actually
predated the other act we just discussed.
The primary use of this is to
address unauthorized access
to computer systems,
networks, and protected data.
It almost sounds like you
can't do the trespass argument.
This is a sounds like
a more reasonable law to test.
Right. What the CFAA
does is it makes it illegal
to knowingly access a computer system
without authorization or to
exceed authorization access.
If I break into
your network or get around
your password right,
to get access to your site
and you have not authorized me that access,
then that becomes the problem.
It doesn't actually say that.
If I agreed to terms of use on
your platform and then access it,
now I'm legitimately there,
Now I start collecting data or using
data outside of what the terms of use allows.
This doesn't really make that clear, right?
That becomes another issue
where people are still
working in the courts
as far as whether that's allowed or not.
But we do know that hacking into
a password protected system
without authorization is against this law.
Also, computer espionage,
which is the unauthorized access to
government computers is outlawed,
trafficking passwords.
So if I am, you
know, sharing, selling passwords,
access codes or similar information so
that you can get
access to a site without authorization.
Right. The trafficker is in the wrong and
then any intentional damage with
the distributed denial
of service attacks is wrong.
So that's where bidders Edge was
considered an injunction because
of the denial of service impacts.
I don't know if they
actually cited this law in
the case or not, but you know,
it's another evidence that it's
another legal legal basis
for denial of service or intentional damage
being being clearly illegal.
What I think is interesting is
when we look at the issue of damages, right,
there is one case that I
think just still blows my mind,
right, is the US.
Versus Arnheim. Arnheim was
initially convicted for identity theft,
so he was accessing
banks without a password, right?
So, in clear violation
of this act we just discussed,
right, the fraud act, right?
He accessed banks without a proper password,
took the identities of
the people and used it for identity theft.
Initially, he was convicted, but in appeal,
that was overturned because
the court ruled that the bank
did not have a sufficient barrier
to entry to prevent public access.
So in this case, not
only is it about
a person accessing a system,
they're not authorized to,
it's on the platform
to make sure that they have
in place sufficient protection
or guard rails to not make it easy, right?
So if you wander into
a site and it's not that hard to get into it,
then you can be innocent.
Qvc sued Result for excessive scraping.
And again, the thing that made them
successful was
citing the denial of service damage.
They said, we can't sell our products to
people because this third party
without authorization is hitting our site so
much that our platform
is not able to function as it's intended.
That was successful damages,
but they typically result
in denial of service,
not issues for privacy or access,
or again, raw data.
It's the denial of service
piece that becomes the problem.
Privacy, privacy becomes very interesting.
European Court of Justice ruled
that individuals have the right
to request search engines,
to remove certain links or
certain information if they're out
dated, irrelevant or excessive.
I don't know that there is
any equiva***t ruling like that in the US.
Sat probably one of the closest ones here.
They settled out of court with
the Federal Trade Commission
over charges that
it was deceiving universe users and
misrepresenting the disappearing messages.
Snapchat is a site where you post
something and then
it's supposed to disappear.
And a lot of people would say things,
thinking that then that message
is gone and it can't be held against them.
They didn't know that Snapchat
was actually keeping the data on
the back end for additional
research or whatever they were doing,
and that was considered misrepresenting it.
Snapchat was considered in the wrong there.
Twitter faced
a class action lawsuit for allegedly
sharing private direct messages
with third parties without proper consent.
That for the case of
marketing or for research
or whatever they were doing there.
These are cases of the platform
being held liable for
not being fully forthright or honest with
users about how they are
using their data and
what they're doing with it.
Maybe the most famous one
is Cambridge Analytica,
which was sued over
illegal collection of data.
What Cambridge Analytica did is they took
privileged Facebook data that they had
because they had a special
relationship with Facebook.
They were then using that
data to manipulate the outcomes of elections.
This wasn't the initial court case,
but it even found they were
bribing certain political officials to,
you know, to promote or
withhold certain information that
shut down Cambridge Analytica and
enforced major settlements
with Facebook over that.
It was because the ruling was that
Facebook had full knowledge
of what Cambridge Analytica was doing,
and therefore they were,
they couldn't hide behind
that law that says, hey,
they're sheltered from the actions
of the users of
their platform because they were considered
complicit with them in the wrongdoing.
In 2019, Youtube settled
with the Federal Trade Commission over
collecting personal identification from
children under 13 without parental consent.
That's part of the Children's
Online Privacy Protection Act,
which is trying to
put in place additional
protection for minors.
But it's interesting that
that's children under 13,
children under 18 generally, requirement.
From legal standpoint,
no copyright infringement.
Generally, the copyright doesn't
protect facts and ideas,
but it might protect original words,
images, logos that express an idea.
Right? Those are the issues
you have to worry about with copyright.
If you're doing scraping,
the rule is you can't burden
the site being scraped.
Most sites are going to have rate limits
where they're going to limit the ability.
You can do this, people get
around that by varying
the attribution of the hits.
They might have multiple tokens,
They'll have one essentially
account that will scrape for a while.
And then when it gets to the rate limit,
then it will drop and then
another token will hit and
it'll start collecting and
then cycle through them.
Or there's other more sophisticated ways
to vary the way the operating system,
the IP
address some of these other things, right?
So that you can continue to scrape
the sites and get
around those rate limits, right?
Those are
a little more sophisticated methods.
But what you have to be
careful of is that you're assured
that you're not affecting
the performance of the site
with your scraping activity.
Also, the scraper cannot gather
sensitive user information such as like
social security number, passport, et cetera,
that can be likened
to data features of
identity theft or personally
identifiable information.
You want to avoid that and
then adhere to fair use laws.
And fair use laws is you can
get the ideas and
the facts which is the raw data,
but you can't necessarily get original words,
images that express an idea, right?
That would be copyright copying
of copyrighted material for
limited and transformative purpose.
Right? Such, I'm going to take
the copyright phrase
and I'm going to comment on it.
I'm going to criticize it,
or I'm going to even do a parody of it,
like that's okay, that
doesn't violate copyright,
that's considered fair use.
The other issue that is
important is when you're talking about,
sometimes people will say
that grabbing data off
of a social media platform
is a source of value for the platform.
And therefore, if you
have agreed to terms of use
on the platform and
you've gotten something of value from it,
that you are now in contract.
But a couple of the issues
is for a contract to be a contract,
you have to have a meeting of the minds,
which means both parties have to have
the same understanding of what is going on.
And there has to be some trade of value.
It's called consideration, right,
if I write a contract with you.
I've never exchanged any
money or anything of value,
then the contract is not valid.
When you sign into a term of
use and then you go onto
the platform, what have you done?
Have you paid a fee to
them for the access to it?
Well, if not, maybe there's
not a contract in place.
It's questionable if you've actually
had to read the terms of service.
Right. Some sites will
require you to at least scroll through
it all to try and create the burden that you
have arguably had a meeting
of the minds and you've read it.
But most contract terms of service are
so long that most people just hit,
yes, yes, yes. They don't really read it.
So they can argue, have
we really had a meeting of the minds,
how we really had a consideration
so that this contract is valid.
They use that with the idea that
I can accept terms of service.
I can get access to a site.
I'm authorized to be there now and now.
I can scrape whatever I
want and do whatever I want with it,
and I don't have to
adhere to the terms of use.
That issue has not been
fully decided in a court of law.
There is no case law that governs it,
and you could argue either side of that.
But I'm not aware of at this point,
any case law that's actually ruled on that
the conditions in which
a social media provider will likely take
action against you is degradation of service.
So, you know, you can look at rate limits in
the Python scrape package, right?
Or providers will have
their own barriers to excessive use.
They'll put captures in there,
these sorts of things.
Because there is a requirement that
the platform has
to do something to make it hard.
But if you're doing
something that degrades service,
the social media provider
may take action against you.
You know, they have to be aware
of potential unwanted scraping,
but then they have to divert
resources to stop you, right?
So you might employ like a non attribution.
You might vary the signals of
scraping to make it a little bit easier.
To make it not so
obvious of what your actions are doing.
Any attacks on reputation
resulting from a publication.
So if you get a site like this,
was a case that we had with Twitter.
If you saw the dormant bot paper
I think we posted in module one,
we found that there were
dormant bots, They never said anything.
They had tons of followers,
this weird pattern,
but the network of
friend followers would manipulate
the visibility of other posts,
of certain posters on the platform.
Well, in doing this, right,
we ended up going and saying, hey,
you need to take these dormant bots down
because they threaten the integrity
of the US elections.
Well, we then go and we got like
an FBI court injunction to take those.
Do that took down 7% of,
of the global Twitter accounts.
And it also was correlated
with about a 23% drop in
their stock price while
they immediately changed their API.
And for about a year,
if you had a Johns Hopkins E mail address,
you were not able to get
a developer API from Twitter.
Now, I'm not saying that that
was super malicious or anything,
but I mean, it certainly looks like that way.
I think any attacks you have
on the reputation of
the platform will probably
motivate them to take action in some way.
It's also important to note that
the rules for the US government are far,
far,
far more restrictive than a private entity.
The US government, regardless
of what you say about Big brother,
the US government has
very strict oversight from Congress.
They have congressional staffers that go
and make sure that the executive branch
and government agencies are not exceeding
their authorities or their permissions
to collect data that
they're not authorized to do.
There might not be a law
against people collecting this data,
but there are often laws
or limitations on what the government can do.
There's congressional oversight,
Congressmen are arguably
answerable to the people.
There's a lot of
different watchdog sites that are out there.
There are some checks and
balances on what the government can
do that doesn't exist for private industry.
So when you actually look
at the likelihood that the government
can collect data and manipulate it to
affect your attitudes and
things that impact your life.
There's a lot of
guardrails and that's really, really hard.
Can Facebook do it?
Can you Tube do it? Can Twitter do it?
Like absolutely, they can do it.
And there's very little laws
or rules that we can do to stop that,
especially when those social media platforms
become monopolistic in nature.
In conclusion, the burden of
privacy is on the platform.
If it's publicly available, it's fair game.
Those are the general rules that I would say.
The platform has to
take the heavy burden
of protecting their data,
preventing access, safeguarding it.
If you can get access to it
without doing too much
extraneous crazy things,
you can probably get it and use
it and it's fair game, right?
This is the game that occurs
then with social media data collection.
In this module, you're going to create
some script to collect social media data.
You have
some pretty wide guardrails to do it.
It's the platform's job to
prevent you from getting the
data that they don't want you to have.
If you're successful and you tell
people about it and you publish it well,
then the platform is likely
to change that API in
some way to make
it either more difficult or not
possible for you to
do what you are able to collect.
That's why these things are always changing.
And that's why we've structured
this module in a way to like
have you experience that pain
as you try and collect data
for use later in the course.
So I'm Dr. Mccall.
I hope you've enjoyed this lecture
on legal cases.
I must include the important
disclaimer that I am not a lawyer.
None of this can be
represented as legal advice.
Any actions you take, you
assume your own risk and your own liability
for what I've attempted to do is just
showcase the key pieces of case law.
This has been reviewed by
Johns Hopkins attorneys to make sure that I'm
not saying anything in
extreme or that's not
like supported by case law,
but it can't be represented as legal advice.
I hope you enjoy this module
and we'll talk to you soon.

Hello, I'm Dr. Ian Mccullough and this
is a lecture on
application program interfaces,
or APIs, specifically those
that are used in social media.
So we're going to review
two common social media API formats,
Json and XML.
And we're going to go into
a little bit more detail in both,
just to start with a brief history.
Right? A lot of this began with
Fielding's dissertation in 2000,
which really brought together
the convergence of networks and software.
And the issue became like,
how do you make your data
available to third parties?
And you might be asking, well,
why would you want to
do that in the first place?
Well, if you think
about the rise of things like
Amazon and ebay, right?
What do you want to do? You want
people to use your platform.
You want people to use
that as their marketplace?
Well, it's really helpful
if people have access
to data and can more
easily interact with your platform,
create their independent applications
that interact and be able to support that.
That's where it really took off.
It was found that if you could
standardize that data,
the formats for which people
are interacting with your platform,
it was a lot easier and people did it at
greater scale flicker you
may have heard when
they were starting with this whole thing,
they didn't pay attention to that.
And so they had a bunch of issues with
the restful APIs between
those kind of early cases.
We found that the easier
you make it for
people to interact with your data,
the more likely they're
going to come to your platform,
the more value you're going
to get out of the platform.
So it becomes really important,
and that's where you've seen
the rise of API's.
But it's really been in
the early 2000 that really took off.
It's, relatively
speaking, a recent phenomenon.
When we're talking about what
are good practices for API,
there's three points that I want to make.
The first one is it
needs to support your business use case.
It's probably not practical
for you to expose all of your data.
You may not want to expose all of
your data for a variety of reasons.
Not to mention proprietary business models,
but also the cost and complexity of
maintaining access and the
API's as data changes.
Being able to think of
the compute and storage
costs that go along with it,
you really want to think through what are
the business use cases for why you
would want a third party
interacting with your data.
Then prioritize those and narrow the access.
You're going to allow for
API access to your platform.
You also want to make sure that
you have detailed documentation.
You don't want to make it hard for people to
interact with your data, access it.
You're going to get some hands on
experience working
with different documentation.
And I'll be curious what
your thoughts are for
the quality of different levels
of documentation on different platforms.
But you want to make it
easy for people to interact with
that you don't want to make it
difficult or people just
aren't going to do it.
Then the other consideration
is to maintain stability.
When you're dealing with API's,
you're going to update your data storage,
various things about your network,
your enterprise,
and you want to try and keep the API stable.
What we find with social media is
there's constantly changing API's.
And that's very frustrating because I
guarantee you any API's
in this course that I have created,
I've done this numerous times.
We get APIs that are
working and then something
will happen and they're going to change it.
Probably the most recent one is when
Elon Musk purchased Twitter,
went in and changed all the API's.
And he did that for a variety of reasons.
But it's very frustrating because
if you have software tools or
products that are relying on
that data and you're using that to
support other clients of your own and
those API's change well,
you have an immediate development cost
of being able to go in and re,
establish that if you're
wanting people to interact with your data,
you want to try and maintain the stability in
the API layers and then have
them access other pieces of your system.
If you're going to change
other areas of your interface,
keep the API stable so that
you're not messing up the third party's work,
requiring them to rewrite API's.
This introduces
a relatively new field
called data virtualization.
And this is a term that allows you
to retrieve and
manipulate data without providing
a whole lot of technical details
about the data, right?
It's these things that augment
or automate your ability to search,
grab, identify the data.
And how are you making it easier
for other people to work with your data?
There's a lot of different solutions
involving AI now.
Some of those solutions augment
human manipulation of the data. There's
Company that I work with connect us
AI that focuses on symbolic AI.
Instead of using
declarative relational algebra,
right?
Like in quel, where you say,
I'm going to take my data,
I'm going to do this to it,
I'm going to add this,
I'm going to join this,
I'm going to move this, right?
And you're just dictating
the procedure for how you're
going to transform the data.
What connects AI does is
we use symbolic AI to
identify first order logic
of relationships between the data.
And then you can just state what
the outcome needs to be.
The AI will describe the optimal path.
When you do that, it significantly lowers
the cost and time to to migrate data,
migrate queries transform data, right?
But these are all things in
the general area of data virtualization,
like how do you make it easy for
people to work with your data?
So we're going to go into more detail on
Jon or Javascript Object Notation.
This is a human readable text, right?
The syntax, The data is
typically in name value pairs
separated by commas.
They'll have curly black brackets
to hold the objects.
Square back brackets or holding arrays,
the variety of different values.
And we'll give you an example here.
This is an example of
Jon for products example, right?
So what you see here is,
you know, ID number two.
What's the name of it? All right.
We have an ice sculpture, there's a price.
There's some tags for searchability,
there's some dimensions, right?
There's a warehouse location,
different object
here for where that's located.
And then we might find like a different ID
here for a different entry.
And you'll see how that's coated up
with similar price dimensions, right?
It doesn't have the same tags,
so you see that that's just not
included in the Json object.
This is a little bit robust for being
able to add different fields like that.
So that's an example of on,
with social media, right?
They sometimes make these available
and I'm going to switch to a,
an example of where you
can see the Json file.
If you look at this red,
this is a red for Johns Hopkins University.
By adding that on extension to it,
you can get the Json file.
I'm going to pull up Chrome
and then I'm going to show you what
that looks like when I
select the Jon link
here you can see what the J here is,
a large Json file,
and this is the Json file
that is associated with the red.
If I eliminate the Json tag here at the end,
you can then see what that file
looks like in its actual rendered format,
non Json format.
As an HTML version.
We look at a reputation example
for Johns Hopkins, right?
We want to store objects on
faculty members and
maybe alumni for faculty members.
You can see that I'm
going to have instructors here.
I'm going to have faculty
and you've got my name here,
first name, last name.
Of course, I teach many of
you know Tony Johnson
who's the program director.
Right. So we have first name,
last name, and of course that he teaches.
Right? So that's an example there.
And then you see similarly
for the alumni over here on the right,
you can see that maybe
we'll have different data, right?
Like the first name, last name,
their degree, their graduation year.
Maybe employer is something
we want to capture.
This is just showing
you how you would structure
the Json object for use in an API.
Now, the XML stands
for Extensible Markup Language,
and it's a different syntax.
What you're seeing is instead of
curly brackets and commas,
we have these little carrots, right?
These little less than or greater than
signs which are identifying.
Here's a message, here's the text,
the message, each one of them.
When you open up message,
you'll see the slash message
at the end to close it out.
And then within that nested,
you'll see text with the slash
to close out text, right?
That's just a different format
for how we would do this.
Here's an example of that
same reputation example for faculty,
and we're just using the XML format.
So you can go back to
the video if you want to look at the way
it was and Json and contrast it with XML.
But hopefully this makes sense
that you see first name,
we close out first name,
we do last name Mccullough,
we close out last name,
Then we see the course.
You see the next one for Tony Johnson.
That's just a contrast between JS and XML.
Here's a table that shows
differences in performance.
If you look at this table,
you'll generally see that Jon is
a more efficient syntax, right?
It a lower memory cost.
It has faster time.
Overall, better performance.
Why then do we still have XML?
Well, many people think that XML
is easier to read, more interpretable.
There's a lot of people that are
more familiar with XML.
When you're thinking about
your development effort.
Do you want to have to spend money
and retraining people to go
from XML to J son?
Is it worth the performance tradeoffs?
And a lot of people are still
sticking with XML for that reason.
I think that over time we're
going to see a greater push towards Jon.
Guys can tell me think nowadays people that
are graduating are becoming equally
familiar with both J son and XML.
I think that that trend
is going to overtake and
people are going to start going
with the language that
has better performance.
Here are some examples of some popular
API's for different social media sites.
They're always changing.
They're changing for a variety of reasons.
They change in response to legal cases
where the public maybe
gets upset at how somebody
has used data off the platform.
So that platform wants to restrict
the API and limit
the amount of data they get,
maybe the value of data,
changes in the marketplace from advertisers
and they want to change it for those reasons.
Or it could be
political reasons or they
just want to be *****. I don't know.
But what I found in the course of,
I started teaching this course in
2015 periodically,
it depends on the platform,
they always are changing.
It's not a very stable API for any
of these when we're
going to start working with them,
instead of me just giving you the API,
we're going to have you guys
take some examples and then go and
work together to try and get some
working API's to be able to
collect data off of these various platforms.
I'm Dr. Mccullough.
This has been a short lecture on API's,
and I hope you've gained something from it.
And I hope you're going to
enjoy getting your hands
dirty with one of our
first hands on assignments.