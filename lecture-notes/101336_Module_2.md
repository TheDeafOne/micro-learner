- James–Lange sequence of events:
  - External event → physiological response (elevated heart rate, sweating, adrenaline) → emotion arises from that physiological state → cortical interpretation follows.
  - Conditioned physiological responses in deep (primitive) brain encode habits and trauma-linked responses.

- Dual-process framing:
  - "System 1": automatic, habitual, emotional, fast.
  - "System 2": deliberative, rational, slow.
  - Self-reports often reflect limited access to System 1 drivers of behavior.

- Coca‑Cola / Pepsi fMRI study:
  - Blind taste tests: Pepsi preferred (higher sugar → greater dopamine).
  - Brand‑labeled tests: Coca‑Cola preferred; prefrontal cortical activation appears and modulates dopamine-driven responses.
  - Marketing implication: emotional/associative branding changes cortical regulation of primal reward responses.

- Dopamine neurobiology and addiction:
  - Primary production areas: ventral tegmental area (VTA) and substantia nigra (VTA dominant).
  - Dopamine pathways project to motor cortex (movement) and limbic reward circuitry (motivation, operant conditioning).
  - Typical dopamine baseline ~50 ng/dL; examples: bad day ~40, great meal/sex ~90, addictive drugs much higher (heroin/cocaine ~400–500, methamphetamines >1000).
  - Chronic dopamine accelerant use → downregulation/pruning of dopamine receptors → lowered baseline (e.g., addicts baseline ~10) → drug use to function rather than to get high.
  - Insula encodes somatic cues leading to conditioned craving (cue → insula → long‑term memory encoding → habit/craving).
  - Social media can produce dopamine responses at cue/anticipation stages and meet biological definitions of addiction.

- Von Economo neurons and social brain anatomy:
  - Von Economo neurons: long, fewer dendrites; concentrated in anterior cingulate cortex (ACC); implicated in rapid signal propagation and linking primal reward signals to cortex.
  - Cerebral cortex surface area correlates with primary social group size across primates.

- Dunbar number and organizational implications:
  - Typical primary social group size ~150 people.
  - Organizations ≤150 allow broad personal knowledge of others; >150 require formal structures to access knowledge/resources.

- Social pain and physical pain overlap:
  - Social exclusion/breakups activate similar brain regions as physical pain (insula).
  - Tylenol (acetaminophen) produced statistically significant reductions in neural markers of social pain in a UCLA study.

- PTSD / military social network findings:
  - Longitudinal psychometric assessments pre-, mid-, and post-deployment in a 1,000-soldier infantry brigade.
  - Peripheral position in unit social networks associated with higher rates of PTSD, depression, and stigmatized counseling.
  - Study data later invalidated/removed because of IRB violations and misconduct; anecdotal operational details: one battalion with denied treatment experienced 18 suicides vs. none in treated battalions.

- Conformity and network centrality (military platoon study):
  - Respect and friendship network mapping; designated central and peripheral nodes.
  - Conformity task with planted wrong answers by confederates on trivia (e.g., tourniquet question).
  - Correlation between centrality and conformity: −0.84 (central actors less likely to conform; peripheral actors more likely to conform to gain acceptance).
  - Normative conformity stronger among socially peripheral individuals.

- Neural similarity and friendship formation (Dartmouth study):
  - fMRI responses to random videos collected from MBA students early in program.
  - Neural-pattern similarity predicted friendship formation over 2–3 years.
  - Correlation declines with social distance: distance 1 > distance 2; negative/near-zero at distance ≥3–4 → concept of "neural horizon."

- Advertising / neuromarketing model:
  - Lewis funnel: capture attention → generate interest → create desire → prompt action.
  - Emotional engagement often more effective than rational argument for driving behavior.

- Neuroimaging and measurement methods:
  - EEG (electrical), fNIRS (functional near-infrared spectroscopy), MRI/fMRI historically used.
  - fNIRS basics: two-wavelength optical modulation measuring oxy-/deoxy‑hemoglobin; surface cortical depth ≈ 2–3 cm; portable miniaturized systems enable field studies but do not access deep structures (e.g., VTA).
  - COVID-era adaptations: facial expression recognition + eye tracking enabled remote neuromarketing without physical contact.

- Facial expression recognition pipeline and neural basis:
  - Visual input → perception → automatic facial mimicry (mirror neurons) → physiological response → interpreted emotion.
  - Computer-vision pipeline: face normalization, augmentation (scaling, rotation, noise), convolutional neural networks → classify emotions (contempt, disgust, fear, happy, neutral, sad, surprise, etc.; ~16 reliably testable classes).
  - Applications: infer deep emotional responses relevant to marketing, engagement, empathy, and motivation.

- Module focus and approach
  - Emphasis on online influence and persuasion grounded in social theory and neuroscience, with some AI/data-science methods used to empirically test theories.
  - Warning against exploratory data-only approaches that risk spurious correlations; recommendation to observe, form hypotheses, collect data, analyze, draw conclusions.
  - Importance of reviewing prior theory and research to inform hypotheses and project feasibility.

- Objectives and themes
  - Explain counterintuitive aspects of social media influence (exposure does not automatically change minds).
  - Describe common theories of influence/persuasion and how online environments moderate them.
  - Examine how platforms and recommender systems manipulate behavior and information exposure.

- Definitions and distinctions
  - Misinformation (fake news): incorrect/false information.
  - Malinformation (truthful but harmful information): true information presented in a damaging way.
  - Disinformation: false information delivered with intent to mislead or change opinion.
  - Propaganda: information presented in a biased way with intent to persuade; can be true or false.
  - Satire/parody: often incorrect or misleading but intended to entertain rather than persuade.
  - Spectrum of problematic content: false connection (misleading headline/caption), misleading content (framing truthful info to persuade), false content (fabricated lies), imposture (impersonation/deepfakes), manipulated content (edited media), fully fabricated synthetic content.

- Key concepts
  - Misperception: holding a false belief.
  - Deception: intentional effort to cause misperceptions.
  - Attitude: affective affinity toward a stimulus that biases processing of information (distinct from factual belief).
  - Belief echo: persistent negative attitudes left by exposure to misinformation even after factual correction.

- Observations about exposure, belief, and correction
  - Not all exposure to misinformation produces misperceptions; people can reject clearly wrong claims.
  - Many misperceptions arise from truthful information framed or selected in misleading ways.
  - Correcting factual errors does not necessarily remove negative attitudes; emotional/attitudinal traces can persist and increase susceptibility to future propaganda.

- Inoculation theory and messaging implications
  - Classic inoculation: present a positive message, introduce a weak counterargument, refute it, and restate the positive message (analogous to vaccination).
  - Risk: repeating the counterargument can create a belief echo, especially if the counterargument is novel or resonates with predispositions.
  - Recommended strategy: inoculate against the source rather than repeat the false content — e.g., identify/attack a vague opposing source, provide a plausible motive (e.g., financial gain), then present the correct information. This reduces the risk of amplifying the false claim.
  - Political messaging has shifted toward attacking sources/character rather than debating issues, leveraging source-based inoculation to create lasting negative attitudes.

- Examples and cases
  - COVID-19: rapid global behavioral change (masking, vaccination) occurred alongside widespread misinformation and politicization; identifying truth vs disinformation was often difficult and polarizing.
  - Russia’s influence operations: use of entertainment, cultural content, and targeted information to shape attitudes in Crimea/Ukraine; malinformation (true but framed content) used effectively to influence opinion without relying on outright falsehoods.
  - 2016 U.S. election: negative truthful or framed information about a candidate used to shift opinions; timing and targets (e.g., primary vs general election) affected outcomes and effectiveness.
  - Misperception example: many Americans believe China owns more than half of U.S. debt and infer incorrect consequences (e.g., repossession of U.S. assets), illustrating how factual data can spawn false inferences that go uncorrected.

- Empirical findings about news exposure and belief
  - Fake-news exposure in a 2016 study: roughly 10–20% of respondents had seen specific false stories, and among those who saw them, a high proportion rated them as accurate.
  - People with skewed information diets (ideologically segregated media consumption) are more likely to view and believe fake news; polarization influences source selection, which then reinforces beliefs.
  - Distribution of news sources: majority of news consumption via direct browsing; search and links also important; social media accounted for a smaller share of general news access but hosted a disproportionate amount of fake news.

- Summary implications
  - Polarization drives selective exposure, which raises susceptibility to disinformation consistent with prior beliefs.
  - Effective countermeasures must consider attitudes, sources, and framing (not only factual correction).
  - Research challenges include defining and bounding the spectrum of disinformation for empirical testing and accounting for emotional/attitudinal persistence after corrections.

- Platform recommendation factors (determine what appears on a user's front page)
  - Engagement metrics: retweets, mentions, favorites, replies, overall interaction volume and tweet popularity
  - Relationship strength: closeness/interaction history with originators
  - Shared connections / network traction: content gaining traction among people in a user’s follower/friend network
  - Recency: time since content creation
  - Media type: video > image > link > plain text for attention
  - Audience behavior: individual user frequency, past engagement patterns, time spent on similar content
  - Geolocation and other contextual signals

- Attempts to reverse-engineer algorithms
  - Direct reverse engineering of front-page ranking is nearly impossible; research used news polling and engineer interviews to identify observable contributing factors.

- Influence operations and amplification tactics
  - Malign actors (e.g., state-sponsored campaigns) push high-volume messaging, abundant images/videos, coordinated accounts, and bots to artificially boost apparent popularity and trick recommender systems.
  - Example estimate: Russia achieves large amplification versus NATO messaging (claimed ~500:1 on some European audiences).
  - NATO/Western actors often constrained by media risk and less coordinated high-volume messaging.

- Firestorm behavior (large negative word-of-mouth cascades)
  - Characterized by rapid spikes, often driven by a small number of high-volume users; media hype and social sharing reinforce participation.
  - Typical firestorms are short-lived (a few days); responding directly often prolongs or reinvigorates them due to counter-argument dynamics.
  - Strategic response: avoid directly addressing the rumor; reframe or inject more compelling content to dominate the conversation.
  - Examples: cancel campaigns (celebrity controversy), hashtag hijacking (#MyNYPD flooded with counter-images).

- Majority illusion and pluralistic ignorance
  - Network structures can make a minority viewpoint appear widespread when high-degree nodes hold that view or networks are assortative.
  - Perception of norms (what people think others believe/do) can diverge from actual norms and drive behavior (college drinking example).
  - Social media influence by celebrities/influencers can create perception mismatches with offline outcomes (appearance of wider support for minority political views).

- Dormant bots and election integrity case study
  - Dormant bots: accounts with many followers but no content generation; positioned to manipulate recommender systems rather than produce posts.
  - Observed anomalous spike in account creations post-January 2017 (many with high follower counts and zero posts).
  - Intervention: reporting led to court injunction and mass banning before 2018 midterms — ~7% suspension of global Twitter activity and ~23% short-term stock price drop; platforms later implemented algorithms to detect/prevent dormant bots.

- Opinion leadership and diffusion/adoption dynamics
  - Celebrity/influencer accounts excel at awareness but are weak at changing personal beliefs or securing adoption.
  - Sociometric (peer/community) opinion leaders, centrally positioned within social groups, are more persuasive for evaluation, trial, adoption, and confirmation stages.
  - Diffusion/adoption stages: awareness → interest → persuasion/evaluation → decision/trial → adoption → confirmation; different leader types matter at different stages.
  - Simple metrics emphasizing national-scale reach miss “last-mile” network effects needed for behavior change.

- Behavior-change framework (heuristic)
  - Behavioral intention (best predictor of behavior) is a function of:
    - Attitudinal beliefs (belief formation, inoculation, belief echoes)
    - Injunctive norms (opinion leaders, normative pressures)
    - Descriptive norms (peer behavior/perceived prevalence)
    - Behavioral control / efficacy (perceived ability to act)
  - Measurement approaches: opinion polls, NLP on content, observed behaviors, social network analysis (required for norm measures).

- Influence tactics grouped by target component
  - Attitudes/beliefs: introduce new beliefs, change existing beliefs, create belief echoes (memes, repeated narratives)
  - Injunctive norms: co-opt/create/remove opinion leaders, shape majority illusion (promote or fabricate influential nodes)
  - Descriptive norms: manufacture apparent peer behavior (synthetic images, fake accounts, coordinated posting)
  - Behavioral control/efficacy: spotlight success stories to increase efficacy; portray victims or helplessness to reduce efficacy

- Manipulative media examples and effects
  - Memes: create persistent belief echoes even after factual correction
  - Deepfakes: fabricate endorsements or co-opt an opinion leader’s persona
  - Synthetic/generated images/avatars: fabricate apparent crowds or support to alter perceived norms

- Methodological recommendation
  - Apply social theory and network analysis to guide data-driven methods (NLP, ML, network measures) so inferences about influence and persuasion are theoretically grounded.

- Narrative immersion (narrative transport) makes messages more believable by reducing counterarguing and increasing identification with characters; brief formats (memes, quotes) can achieve this.

- Functions of stories:
  - Camouflage persuasive arguments (presented as entertainment).
  - Create cognitive dissonance by letting audiences identify with characters who hold counter-attitudinal beliefs.
  - Capture attention and move audiences through an advertising/marketing funnel.
  - Explain identity and social norms, define heroes/villains, and justify actions via moral framing.

- Four basic criteria for testing plausibility of a story:
  - Probability: internal plausibility within declared parameters (setting, rules).
  - Consistency: consistent character behavior and narrative logic.
  - Fidelity/Fact grounding: inclusion of some factual anchors improves believability.
  - Coherence: characters, events, and facts must cohere.

- Example: “Pizzagate” — false basement-sex-ring claim lacked factual basis but had narrative probability for believers, leading to real-world action.

- Cognitive and social biases affecting narrative uptake:
  - Confirmation bias and my-side bias preserve preexisting beliefs.
  - People accept minimally sufficient pro-attitudinal arguments and reject counterevidence.
  - Emotional decision-making dominates over deliberative reasoning; the brain avoids effortful evaluation.
  - Attention is required before argument processing; entertainment formats and sensational content capture attention.
  - Conspiracy beliefs cluster; belief in one conspiracy predicts belief in others.
  - Fake news exploits prejudice, anxiety, fear, and attention dynamics; attention creates ad revenue incentives.

- Social media and information dynamics:
  - Echo chambers and abundant media sources increase perceived authenticity of peer-generated content.
  - Viral sharing and dopamine reinforcement encourage repeat sharing.
  - Platforms reward attention-grabbing content (ad funnel logic: attention → interest → action).
  - Countering false narratives can backfire by generating further counterarguing; correction sometimes strengthens belief.
  - Redirecting/overwhelming strategies (flooding attention with alternative content) can suppress unwanted stories (example: distracting coverage after a crisis).

- Effective counter-narrative design principles:
  - Must build on existing beliefs to gain traction.
  - Trusted messengers improve acceptance; perceived motives of promoters strongly influence reception.
  - Counterarguments often attack the source or portray information as a weapon.
  - Deepfakes exploit trust by impersonating authoritative sources and can complicate counters.

- Political messaging observations:
  - Criticism often targets personalities (speech, mannerisms) rather than specific policies.
  - Perceptions of institutional power are biased by partisan alignment (oil-price study: views of presidential influence flipped with party in power).

- Narrative-structure elements (Haven / DARPA Narrative Networks):
  - Characters: main/identity character plus supporting characters; identity character is key for transport.
  - Traits: character descriptors that shape audience attitudes.
  - Goals: tangible objectives for characters; strong contrast between success/failure increases engagement.
  - Motives: culturally appropriate reasons for pursuing goals; motive–goal matching creates identification (stronger than trait matching).
  - Conflicts/Obstacles: tangible antagonists and high stakes increase engagement.
  - Struggles/Sequence: events and actions leading to climax.
  - Climax: decisive scene where success/failure occurs; identity character involvement increases perceived efficacy.
  - Resolution: emotional imprint and call-to-action; unresolved endings are forgotten, positive endings produce lower arousal/longer positive effect, negative/unfair endings produce high arousal and stronger calls to action.
  - Imagery and detail: used to draw people in; level of detail affects assimilation vs. contrast (low detail → greater assimilation; high detail → more critical comparison).

- Practical guidance for influence design:
  - Emphasize goal–motive matching and familiar settings to create identification.
  - Limit overly specific detail when seeking broad assimilation.
  - Ensure antagonist is clearly preventing the protagonist’s goals.
  - Design endings with desired emotional arousal to prompt action (use negative outcomes for urgent calls-to-action; positive outcomes for sustained goodwill).
  - Preserve audience efficacy by making the identity character’s success linked to the audience’s potential agency.

- Neuromarketing / SalesBrain insights:
  - Effective ads/memes should trigger primal/emotional responses and then engage higher-order decision processes.
  - Eye-tracking shows faces draw primary attention; expressive faces evoke mirroring responses.
  - Six tuning dimensions for ads/memes: personal, contrast, tangible, memorable, visual, emotional (use these to increase effectiveness).

- Research approach:
  - Treat narrative elements as experimental variables (identity level, goal clarity, detail, etc.) to statistically measure influence and optimize message design.

- Core concepts
  - Social judgment theory: attitudes lie on a spectrum with latitudes of acceptance, noncommitment, and rejection; persuasive messages landing in the latitude of rejection produce a boomerang effect (recipient moves further from the advocated position).
  - Boomerang effect: strong counterattitudinal arguments can increase polarization and entrench opposing views; corrections can produce short-term gains but long-term backfire.

- Illustrative example (death penalty)
  - Person A: accepts life without parole or death penalty; rejects light sentences and extreme torture.
  - Person B: rejects death penalty; accepts life with parole; rejects fines/no prison.
  - If A uses extreme pro-death-penalty arguments, they land in B’s latitude of rejection and push B toward greater leniency.

- Rumor/misinformation correction (Adam Bernski study, Obama birth certificate example)
  - April 2011 release of birth certificate: pre-release polls — 55% thought Obama was a U.S. citizen, 15% thought he was not, many unsure.
  - Immediately after release: more people believed he was a citizen; many "not sure" moved to "citizen"; slight decrease in "not citizen."
  - A year later: same overall citizen count but an increase in people believing he was not a citizen, especially among Republicans — evidence of a backfire/boomerang effect over time due to practiced counterarguing.

- Neuroscience findings on persuasion and prediction of behavior
  - Medial prefrontal cortex (mPFC) and posterior cingulate (PCC) activity correlate with self-integration and predict behavior change better than self-report.
  - Lateral prefrontal cortex (LPFC) activity corresponds to counterarguing and is negatively correlated with mPFC activity, disrupting behavior change.
  - Rhetorical persuasion consistent with prior beliefs boosts mPFC and behavior change; counterattitudinal info increases LPFC and resistance.
  - Neural responses (e.g., mPFC activation) predict real-world outcomes (e.g., call volume in smoking-cessation ads) more accurately than self-reported ad liking.

- Neurocognitive influence model elements
  - Output: behavior change.
  - Pathways: self-integration (mPFC/PCC) promotes change; counterarguing (LPFC) prevents change.
  - Interventions that suppress counterarguing allow self-integration to influence behavior.

- Mechanisms to disrupt counterarguing
  - Self-affirmation: activates ventral striatum and orbitofrontal cortex (reward circuitry, dopamine), reduces counterarguing, increases self-integration.
  - Narrative immersion (story transport): activates dorsal mPFC and temporal-parietal junction (empathy/identification), reduces counterarguing.
  - Distraction: can also reduce counterarguing; musical immersion similarly effective.

- Field research applications and methods
  - Use of functional near-infrared spectroscopy (fNIRS) as a portable alternative to fMRI to measure dorsal mPFC and related activation in field settings.
  - Study in Jordan, Egypt, Iraq, etc.: mPFC activity predicts ad effectiveness beyond perceived effectiveness; evidence of negative correlation between frontal pole (self-integration) and counterarguing region (reported correlation ≈ -0.17).
  - Tobacco ad study with adolescents:
    - Metrics: frontal pole asymmetry (engagement vs. avoidance) and right LPFC (message resistance).
    - Findings: nature/eco-themed ads were engaging but met skepticism about company motives; "more is better" nicotine-boosting ad generated high merchandising appeal but avoidance and lower actual sales; organic/eco (green) ads tended to increase engagement; social/friends-themed ad produced highest engagement/motivation.

- Practical implications and patterns
  - Misinformation is sticky: initial exposure forms a baseline that frames subsequent information.
  - Corrections often show short-term effectiveness but can fade and backfire over time (belief echoes, boomerang, practiced counterarguing).
  - Inoculation strategies that prime counterarguing can be protective but may also strengthen resistance if not paired with approaches that reduce counterarguing.
  - Strategies that widen latitudes of acceptance, use affirmation and narrative immersion, or avoid extreme counterattitudinal messaging are more likely to reduce boomerang and produce behavior change.
  - Intuitive responses (e.g., publicly correcting false claims) frequently fail and can worsen the situation without careful reframing and techniques that suppress counterarguing.