Stephyn Butcher
00:02
Welcome to Artificial Intelligence. This is the first of three lectures in this module that cover, sort of, the high-level concepts.
Some of the history of artificial intelligence, and set the framework for the rest of the semester.
So, in the past, I sort of ignored these chapters, but they are sort of important for you to understand in terms of having a framework of how Norvig and Russell
we'll talk about the various things, so…
This is not a set of lectures about algorithms, it's not a deep dive into algorithms. It's more of a high-level view of the concepts involved, and how people think about artificial intelligence, the history of artificial intelligence.
And as we'll see, we're bringing ethics up front to the first week, rather than sort of tacking it on as an afterthought.
So, our roadmap here is to talk about
At least the way Norvig and Russell frame it.
for possible approaches to AI, think about what AI is. Their standard model is rational agents, and…
Norvig and Russell believe this is… the scientific approach to AI would be to concentrate on rational agents. We'll talk about two refinements to that standard model.
Then we'll talk about, sort of, the intellectual roots of AI.
some of the other fields that go into AI and the way that we think about it. We'll talk about… briefly about some of the…
history of AI and the cycles, there's a lot of cycles in AI, we're probably in one now.
That sort of start off with hype.
And, … and then end in tears, and then people sort of have a sobering reflection on what we learned, and they move on to the next thing.
So, we'll talk about how we've sort of moved from logic being the focus, probabilistic AI to being the focus, then learning to being the focus. We'll talk a little bit about applications, evaluation, and benchmarks, risks, ethics, and the long-term control problem, maybe a little key takeaways and food for thought.
But first, something that's not really covered in Norvig and Russell is this idea of systems AI versus Internet AI. And a lot of the things we're likely to learn in our machine learning classes, especially, are really sort of internet AI. It's not that they don't fit into Norvig and Russell's framework.
But in systems AI, there's a lot of focus on agents, and these agents perceive and act in the environment, so it's very much rooted in this idea of an artificial intelligence.
But this artificial intelligence is maybe embodied.
There's a broad framing of perception, reasoning, learning, and decision making, and probably the best example here is an autonomous robot.
But maybe also some game-playing programs, planning systems are, sort of in this bucket.
Internet AI is probably what most of us encounter, it's most of us what we encounter in our jobs, maybe it's most of what we encounter in our classes. We have a machine learning model, it takes inputs, it generates outputs, it's a classification, it's a prediction, and maybe it's embedded in a larger system, so…
We have, structured data, we have the model, we have an output, prediction of score, will this customer churn? What's the probability of this customer churning? Good example, or is this spam? What's a good recommendation? Is this transaction fraud? So, we have a single model that we've trained.
We insert it into a larger system, just to give that system
some smarts, and that's kind of what some people are calling internet AI. And that's probably the largest application, but if you don't think of this other idea of agents.
maybe possibly autonomous agents. A lot of the framing for the way Norvig and Russell discuss things is gonna be slightly off to you.
So Internet AI just makes systems smarter, but it doesn't necessarily fit neatly into this idea, oh, we have an agent with persimps and actions. I just want you to be aware of that. But given that we are talking about systems AI, what are the four different ways we could talk about this?
So, Norvig and Russell break it down basically into four things. It's a quadrant, a 2x2 quadrant.
We could have systems that think like humans, we could have systems that act like humans, we could have systems that think rationally, and we have systems that act rationally.
And they identify this last one as the standard model.
We don't necessarily sit… we don't necessarily care how…
We model the action, it doesn't have to be faithful to human biology, it doesn't have to be faithful to any particular criteria, it's just the result has to be a rational action.
One thing to think about is where do you think Gen AI fits in this? Where do you think, in this quadrant, where's Gen AI?
So let's compare acting human, or humanly, versus acting rational or rationally. So acting human might be just imitation. We would have, then, so maybe this falls under the Turing test.
imitation. Acting rational means that we have some standard performance benchmarks, we have some metric where we can say, this is the best that can be done.
And we compare the agents Performance to these criteria.
And for each of these, there varies. This is somebody just, somebody quipped, and one of my professors, I think, quipped a long time ago that this is engineering because there are trade-offs, and there are going to be trade-offs in terms of transparency, robustness, optimality.
It's important, then, to keep separate some of these, concepts and what they actually cover.
The first is the agent, so the agent perceives and acts in the environment. That might be the whole kind of…
Hmm… being.
The agent function is an abstraction over…
A behavior, so where we go from a percept history, so what were all my inputs, to some sort of action.
And then…
There's some sort of performance measure against the external evaluation of the behavior, not necessarily of the manipulation of the percepts.
So this is sort of the standard model of AI. Again, this is Norvig and Russell. There are people who are working on, say, embodied AI.
Which is… comes out of the idea that intelligence is situated in your body, not just in your mind. They sort of reject the Cartesian mind-body dualism. I think this almost falls outside of Norwig and Russell's quadrants, but we're going to take Norwig and Russell as given.
The standard model of AI has a rational agent which chooses the action that maximizes expected performance, uses available information, beliefs.
About the world, so beliefs in terms of percepts.
It has objectives, these objectives are maybe measured in terms of utility, you can think of that as a loss function, error function, and it has actions and policies. And again.
Some of these things, like when we talk about reinforcement learning later in the semester, you go, oh yeah, this makes perfect sense.
We talk about linear regression, as we will in the third module.
Or the linear regression module. It may not be the third, You're gonna, like, put…
how is… how is this utility? How… where… what are beliefs? How do beliefs apply here? So, it's not always gonna fit, sort of this framework is in the background.
The standard model handles uncertainty and trade-offs.
We refine this model by recognizing bounded rationality. So, we'll talk about this a little bit later, but in economics, there's the idea of perfect information. Ironically, in economics, people are modeled as rational agents. There is the assumption that everybody always acts rationally.
And, … If they don't.
act rationally, it's because you don't understand their information and incentives, you know, you don't understand what their constraints are. But there is a recognition that perfect information is unattainable.
So we might have something called bounded rationality. It's also been called satisficing. So we can't optimize, but we can satisfy. And so that's an approximate perfect info… it's an approximate rationality under imperfect information and other
Resource constraints.
Refinement 2 is this idea of uncertain human objectives, so fixed, fully known objectives are unrealistic. Our systems should be benefiting humans, we may not be able to specify what the true objectives are.
So the implications are, is that these systems, and again, we're talking about big systems, so you have to kind of pop your mind out of this idea of, like, the canonical linear regression internet AI. We're talking about something bigger.
might want to do preference learning, and might require assistance, and might, you know, need courageability, so…
I think this might be one of the missing positives, if you know that incorrigible is somebody who can't be instructed.
Corrigibility means something or somebody that is instructable.
So…
as we look at the history, foundations, the framing concepts in AI, we can, we can think about the philosophical roots.
So, the philosophical roots, I think, of AI are definitely set in the Cartesian dualism of… Mind and body.
Dualism. And that the mind is…
machine, it's based on reasoning, this reasoning is symbol manipulation. We have internal representations of things… symbols.
With, we apply logic and inference to them, and we deliberate. So these are probably the foundation philosophical philosophical roots of AI. It doesn't change a whole lot now as we add math.
So math is sort of our way of…
Ultimately, it's all computation, because we're talking about computers.
But, we move from entailment, proofs, knowledge representation, maybe probability and certainty-based theorem, expected utility.
And then we end up with computation algorithms that work over these things, or use these things, even when we need to talk about tractability, complexity.
So, for example, the importance of math, we can't… we cannot underestimate
the impact of linear algebra and then GPUs on the current generation of Gen AI algorithms, like…
Math is just the foundation for those things, but…
Gen AI is a very, very small subsection of all of AI.
Very small.
As I say, again, when we think about this systems approach to AI, economics and decision theory is very important.
preferences can be expressed as utilities, beliefs are expressed as probabilities. In that sense, we have expected utility maximization is sort of…
the overarching theory of how we model the behavior of our AI agents.
And then the behaviors will depend on trade-offs in the environment, the percepts we get, trade-offs in the actions that the agent… sometimes I'll say we, and I, and you.
when I am anthropomorphizing the agent as, like… and it's a very good thing to do when you are thinking about things, as well as putting your head in the mind of the agent. So, sometimes I say, you, we, us, me.
As the agent, so as…
you, as the agent, have to think about trade-offs and attitudes towards risk. Attitudes towards risk, again, is
Will affect your expected utility maximization.
There is inspiration from neuroscience and cognitive psychology, but I think a lot… not as much as there should be.
I think the main issue there is there is a very large and important mismatch between the brain's biology and computers.
Our artificial neural network analogies are a little bit overdrawn and exaggerated.
linear… the brain does not work by linear algebra, or really any approximation to it. And I think, actually, the…
philosophical…
mind-body duality holds more weight than, here, neuroscience and cognitive psychology. We just sometimes use elements from these fields, but probably not as much as we should.
Linguistics and language are important.
Language has a formal structure, syntax, semantics, pragmatics, language is… contains information, and information is something that we want agents to work on. And so there… there can be a bridge between
beliefs, and…
knowledge representation and language, and we're really just starting to see this, really, with GenAI and things like ChatGPT, where you have things that
Just are very, very skillful at manipulating language, but say things that are absolutely wrong.
So, we're… we're just gonna try, in the coming
Years, we're gonna have to spend a lot of time working on this issue.
But, language itself has always been important.
for AI, even from the, sort of, symbolic AI days.
Of course, since we're talking about computers, computer hardware capabilities and software engineering capabilities have always been important.
Hardware tells you what you can do at scale, how fast. Software engineering determines how maintainable and reliable these systems are. Platforms and tools make it easier or harder to do these things. They influence reproducibility and deployment.
One interesting side note is, Hardware can make things…
feasible, that seemed infeasible in the past. So, I'll talk about this when we talk about planning.
Control theory is obviously very important, so control theory predates AI, and it was used… it has been used a lot to think of things about optimal actions, especially in the case of robotics and reinforcement learning.
So, a brief history of AI that has been an ongoing cycle of… pipe.
Disappointment and sober reflection and synthesis.
Things tend to be… we integrate our mistakes, but for some reason, we just always end up with…
… you know, symbolic logic… symbolic AI was going to… we solved the problem.
And it turned out that we hadn't, in fact, and that, you know, symbolic AI, it turned out it was really easy to come up with a symbolic system that could do…
Undergraduate calculus, but you couldn't come up with a system that could distinguish a cat from a dog in a picture.
We would have thought that the opposite would have been true.
And then we started out with the first, artificial neural networks, and we hadn't learned about the back prop algorithm yet, and so there was another… I wouldn't say nuclear winter, but no, AI winter.
And even in, machine learning, there have been fashion support… vector machines were going to save the world, and everyone was using them for everything.
And then random forests were gonna save the world, and everybody was using them for everything. And this just sort of… this changes, and even now, this is 2025,
Sam Altman is backing away from what he thinks that ChatGPT-5 can do.
So the first sort of… phase was logic systems, symbolic AI systems.
that, were very fragile. They were hard to engineer, so you had to have people put them together. We had the algorithms, and we'll see them in the semester, to manipulate them and use them, but you still needed people to put them together.
And they often fell apart in an open world.
Probabilistic models got over some of that, and we started to learn from data rather than having handcrafted knowledge.
So, one thing, though, that, it's not… there's no free lunch.
The problem then with learning from data is, is you only learn from that data.
And you have the issue of distribution or concept drift, whereas if the world is changing, the distribution of the data you learned from is changing, and your model fails to work.
In this new world So…
Changing open worlds actually ends up affecting both of these approaches, it's just maybe the learning happens further down the road.
Tons of applications.
Vision, language, planning, robotics, recommendations systems, fraud detection, science, you know. So we have to sort of…
Think of it again, you know, internet AI, there's just tons of different ways that you can apply even something as simple as a logistic regression.
And then, …
We have to think about, other specific applications, sort of foundational applications, vision, language, robotics, planning.
And then we have to think beyond accuracy, we have to think about fairness, safety, calibration. Calibration, again, has to do with concept drift.
And, machine learning algorithms.
We have to think about ethics, and risks, and governance.
You can think about bias, discrimination, privacy.
We have to think about safety, reliability, and misuse.
And it's hard, because our profession is not like doctors. We don't have a Hippocratic Oath. We don't have, certificate…
Kating bodies, where, you know, a hospital can't make a doctor do something that would make them lose their certificates or accreditations.
But if your boss tells you to do something that you think is blatantly unethical, and you say… you can't say, no, I have taken some oath to not do anything unethical, they go, okay, you're fired, and they'll hire somebody that will… that will do it.
So the incentives for ethical behavior are not always there. We do really need transparency, accountability, and oversight for these systems, especially these big systems.
We need to think about long-term control and enlightenment, alignment.
advanced systems can generalize in unanticipated ways. I think that's sort of happened with GenAI. We…
People will often say that we don't know how ChatGPT works, that's not true. We do know how ChatGPT works. We don't know why it works so well.
And I don't think anyone thought that it would be able to do what it does.
Even given the words.
And, there is sort of a problem where… If you don't have…
If you're not correctly specifying and constrained the ethical portions, what it can and can't do, Then…
these tools or leverages, so deepfakes, for example.
If you We have this tool now for deepfakes, and, you know, there's just…
Anybody has access to this technology now, where they can make a video that seems to be somebody saying something they didn't say.
So, in this particular subset, there are research directions in terms of preference learning, think about uncertainty over objectives, cordability, constitutional constraints, you know, some of this stuff they attempt to control.
If you look at the basic system prompts for ChatGPT, Claude, and other things.
So our key takeaway is that in Norvig and Russell, there's a unifying lens of rational agents under uncertainty and constraints. So whenever you're reading a new chapter, and he talks about agents, and the agents trying to do this, and the agents doing that, that's the framework.
AI draws on a lot of different fields.
So, it draws on logic, probability, computation, decision 3, economics, and other things.
Progress comes in cycles. We're in one now.
And ethics and control should be designed into these systems, not added on as afterthoughts.
So I just want you to think about some of these questions now before you go to the next lecture. Where would you place large language models in the 2x2 matrix of think versus act, human versus rational?
Can you give an example of an objective that's easy to state, but hard to specify, hard to implement?
And … I might have over… overshared on this one, but what… where do you… where do you think we are in our current cycle, and why? So just… just things to think about.
I'll see you in the next lecture.Stephyn Butcher
00:03
Welcome back.
In this lecture, we're going to cover the material in Chapter 2.
We're going to dig a little bit deeper into the idea of the rational agent. Last time, we talked about
Norvig and Russell's 2x2 grid, and for them, their standard model is an agent that acts rationally.
And we're going to dig a little bit more into that agent model.
Again, this might be something that we're not used to thinking about. We're not used to thinking about things this way, because we tend to work
more with internet AI.
Which is, you know, we build a model, and we insert it into some other larger system, maybe, again, to make recommendations.
Or to predict customer, to customer churn, or to predict fraudulent.
transactions. So, again, we… Norviget and Russell here in Artificial Intelligence are just… they're thinking about really big ideas.
So here's our roadmap. We're going to talk more about agents and the agent function and performance measures. We're going to talk about Ps.
Which is the way they sort of think about the environment, environmental dimensions, well, PEAS is not just the environment, but it's the…
the partitioning of the agent and the environment and the different parts. Then we'll talk about the environment dimension, objective uncertainty, different agent program designs, learning agents, generative AI, key takeaways.
So, again, this is not a deep dive into algorithms, that's actually what will come in the next module. This is sort of getting our feet wet, getting ourselves oriented towards the way that the authors think about these topics.
So what's an agent?
An agent is… something.
I always think of it as being kind of like this blob, kind of a Disney thing with eyes and a…
computer tape thing for a mouth, and maybe he only has one wheel, and a certain antenna going out, and maybe some arms. So it's going around in its environment. It always… I always think, whenever I think of an agent, I always think of a robot, some kind of robot.
And it perceives and acts in the environment. It receives percepts, which is kind of just fancy for input via sensors, and it acts via its actuators. So, for this agent in my mind.
I could think of it in terms of, …
It can see with the eyes, maybe it has auditory microphones for hearing.
There may or may not be tactile sensors in the hands, maybe it has GPS, and with its little antenna, it can receive information from elsewhere. Actuators are the wheels, and the hands, and maybe the whole body, so that it can turn to look at different things. So, that's kind of the agent.
Agent function is an abstraction, so it maps, again, percept histories onto actions. It's an abstract definition separate from implementation. It can be deterministic or stochastic.
Which… deterministic just means with the same percept history, you always get the same actions. Stochastic means with the same percept history, you could get different actions. There are various reasons why you might want that. We'll talk about that, throughout the semester.
We need a performance measure, something that quantifies what successful agent behavior looks like.
It provides the basis for rational action. It defines what rational action is, but it's environment dependent.
Putting these together, then we get the rational agent, which chooses actions to maximize expected performance, again, on the performance metric, and it uses a percept history, knowledge of environment to do this, and is able to handle uncertainty, sort of at best, so…
we'll see with the agent models that this isn't always… like, this is our aspiration. Not all agents can actually do this.
So, Norvig and Russell sort of divide the task environment into P's, so some sort of performance measure.
The environment, actuators, and sensors, so…
in a design-first sort of way, maybe this is the… if you're familiar with software engineering methodologies, this is definitely waterfall.
method, not agile. We're going to clearly define our P's. There's an example, taxi. So, performance, we have passengers delivered on time, customer satisfaction. Notice there's two measurements here. This mixes in a multi-objective optimization problem.
The environment is the streets.
Traffic lights, passengers, other cars.
Actuators are acceleration, braking, steering, fare processing.
sensors, maybe GPS, cameras, LiDAR, passenger requests, so maybe there needs to be some sort of, language, natural language input.
Environmental… environments can… environments vary based on different dimensions.
So, for… we can have fully versus partially observerable, so in a fully observable environment, we can see the entire system state.
For partially observable, we can only see part of it.
We can have a single agent versus a multi-agent environment, so… taxi is a good example of multi-agent. We have our agent, maybe the self-driving taxi cab, but there are other cars in the road, there are the passengers, other things.
The other agents can either be cooperative or competitive.
We can have deterministic versus stochastic. Deterministic just means
the successor state is currently… is completely determined by the current state. Like, we know the current state, we know the actions, we know what the successor state is.
Stochastic means it's not. Now, from my point of view, this is another way of expressing fully versus partially observable. If you have something that is fully observable, it must be deterministic.
And that anything that's stochastic must of necessity be partially observable.
But not everybody breaks it down that way. If you recognize it, that's a Bayesian point of view.
And, it's still useful to segment these and keep them apart rather than collapsing it.
There's episodic versus sequential. So, episodic means percept action, and that's it. That's one episode. Sequential means there's percept, action, percept, action, percept, action, percept, action. So, you can actually see where internet AI is episodic.
We get structured data, that's our input.
We do a classification, we do a churn score, we do a fraud score, and that's it. That's the episode.
Sequential is gonna be more different, tasks that follow on each versus each other.
We can have worlds, environments that are static, we can have worlds that are dynamic, they can have discrete measurements versus continuous measurements, they can be unknown or unknown. So, this is all kinds of different dimensions.
So, if we think of chess?
It's fully observable, deterministic, I think these often go together. Two-player, so it's multi-agent. Sequential, so it's not episodic, it's discrete, we don't have continuous states of where the queen and the pawns can be, and we fully know all… everything that goes on.
Self… Driving car, partially observable, stochastic, so again, these tend to go together. Multi-agent, sequential.
Continuous spaces, dynamic and partially known.
I don't…
So, Norvig is going to talk about objective uncertainty. So, a performance measure may be unknown or hard to… well, so this is… this is a different objective. So, this is uncertainty in our objectives, not
Objective uncertainty.
So, uncertainty in our objectives states that performance measures can be unknown or hard to specify. We have a risk of optimizing the wrong objectives.
And we can, we need to be able to design agents with uncertain… when there is uncertainty over true goals.
And again, these are big picture things. These are things like, when we get in the nitty-gritty.
Individual algorithms we're implementing may completely ignore any of these things.
So, these are sort of the big ideas into which a lot of our later engineering will fit, but our later engineering won't.
address them specifically.
Because these systems are really hard.
So, the agent program is the implementation of the agent function. Remember, the agent function is abstract.
It varies by the information decision strategies, there are trade-offs with efficiency, flexibility, compactness, so everything…
The difference between pseudocode and implementation…
That you are probably familiar with applies here as well.
So let's look at some different kinds of agents. So there's the simple reflex agent, responds directly to the current percept, has no internal state, works in a fully observable, simple environment.
I would say that this is probably Internet AI.
It's just simple reflex agents, so we can add to this being episodic.
So the sensors are basically, we get the structured data, the logistic regression says, this is the churn score.
Maybe it's not conditioned, so maybe it's a model. We could just put a…
model-based here, but we don't have state, so there's probably something here that Norvig and Russell miss between a simple reflex, which might use a model.
That we learn from data instead of condition action rules, so we can have simple reflexes that are math and not logic.
Here we have model-based reflex, but he adds in not only a model here, condition action, but he adds in state. So we have sensors to get the information, what's the world like now, we look at the state, we think about how the world involves, what do my actions do. You can already see, like, this is super complicated.
This is super interesting, but it's far beyond a lot of things we can actually, most of us build.
And, again, there's condition action rules that say, what should I do now? That affects the actuators, affects the environment, which affects our sensors. If we're sequential, if we're episodic, then these two are disconnected from each other.
We could have a goal-based agent.
So… we have sort of the same things, but it's like, you know, what do my actions do?
what would it be like if I did this action? What are my goals? What should I do now? What should I do now kind of ends up being…
the, … where am I?
symbol or model. Symbols or math. Sometimes we call it GO-Fi, good old-fashioned AI.
Even though we may call it good, old-fashioned AI, it's still in use. So, symbolic AI or numeric AI is normally the way that I'll frame it.
And then there's utility-based agents. This is kind of the highest level. And again, this is very sort of abstract, and a lot of us don't get to work on these systems, but these are really good ways of thinking about these things. So we can go, what is the world like?
Now, our sensors tell me what is the world like now, we update our state.
We have a model of how the world changes, what it would be like to do if I did action A, and then there's sort of, how happy will I be in such a state?
And therefore, what action should I do now? So instead of goals, we have utility.
And then…
at the top is, top, top level, we're gonna end up with learning agents. They can learn over time. They have sensors that go to, sort of.
a, …
Well, this is actually built on top of the other one, so the performance element, this is sort of one of those other agent types, is the performance element.
And now we have other aspects that allow it to learn and get better.
Of course, some of this is not going to work for, like, a reflex agent, because we can't… it can't learn. Like, there's no reason to have a learning element here.
A lot of times, this is sort of mis… specified, so again.
When we talk about learning and learning from data and learning agents, there's sort of this mismatch between
Most of the time, what happens is that we have data, we train a model on it, so this is canonical, sort of typical machine learning.
We train a model, and we use it.
And that's not a learning agent, because it's sort of not learning on the job. Like, we learned it, it's fixed. If we need to adjust it, we need to retrain it, come up with a whole new model.
So that's not what's envisioned here. This is not machine learning.
This might be machine learning in here. Maybe it holds onto the data, it gets more data, retrains the model inside, and that's sort of all encapsulated in here.
So, we hear a lot about generative AI agents today, and agentic AI. So, where do you think that fits into Norvig's scheme of agents?
So, you think about that.
Our key takeaways here is that an agent is just
you know, perception to action mapping in the abstract. So maybe this is the agent function. Agents themselves can have different forms. We have the P's framework for talking about environment task design.
There's different environmental properties which will determine our appropriate agent design. Rationality is defined relative to a performance measure. You change the performance measure, it changes what rationality is or means.
Learning can improve an agent over time, but that's only in those agents that are set up to learn over time, to change their models. And a lot of internet AI that we associate with machine learning is not actually a learning agent.
And GenAI kind of expands some of our traditional concepts, but maybe not.
So, some food for thought here, again, is to identify a task, some task.
and specify its P's. What would you want to do? And this can be handy even if you're trying to build a Gentec AI using LLMs.
Specify the P's element, see how this all works, then think about what type of agent works best for that task, and then another thing.
to think of how generative AI extends or complements traditional agents.Stephyn Butcher
00:02
All right, welcome to this lecture on Chapter 27 of Norvig and Russell's Artificial Intelligence Modern Approach.
I… want to bring this forward sooner rather than later. You notice it's chapter 27, because I think the… the topics are important.
And, we don't sort of want them to be an afterthought.
At the same time, as I sort of mentioned in the previous lectures in this module.
They can be a little bit abstract, and as we do start our algorithm deep dives for the rest of the semester, it will seem like they don't super apply.
And so, you just always have to keep these… you have to…
Keep them in the back of your mind, but bring them to the fore.
when you're designing the systems, not necessarily implementing the algorithms. So, the time to think about ethics is as you're building the system and deciding what data to use, not necessarily when you're picking which
Algorithm to use.
So, this should be sort of upfront stuff.
We are going to talk a little bit about philosophy as well. This is a little bit of a grab bag, and … so let's start.
So we're going to talk about weak versus strong AI, Turing tests and chatbots, consciousness, AI as a powerful technology, ethical imperatives, fairness, trust, and transparency, automation in society, generative AI ethics, and
Do some key takeaways, and again, some food for thought discussion points.
So, the first thing is the difference between weak AI and strong AI. So, weak AI basically says that machines can simulate intelligent behavior.
They make no claim to actual consciousness, and they're simply functionally useful.
That's all Norvig and Russell really try to do. They're in their quadrant of acting rationally.
Strong AI suggests that machines can have genuine minds, consciousness, understanding, self-awareness, and intentionality.
There is a physical, physical, philosophical debate that's ongoing, and this is starting to come up.
with ChatGPT and Gen AI models, where people are saying weird things, like it's, like, 1% consciousness, and I don't even know how you measure that.
I am going to assert that
No. ChatGPT is not any form of strong AI.
Turing reframed, reframed this in terms of behavioral
In terms of behavior. So he said, can machines think?
So, we replace that with observable behavior. That's the Turing test. Can the evaluator distinguish a human from a machine in conversation?
There were objections which he anticipated, some of them were weird.
At the time he's writing, he had to dismiss the possibility of ESP.
Which seems weird. And there are some questions as to whether ChatGPT has already passed the Turing test.
But at the same time, at this point, ChatGPT has been proven not to develop internal models, not to be able to reason. So, it's not clear that we can just go off of behavior.
I think that this is reopening the philosophical discourse.
So…
Again, chatbots have passed the Turing test. They, at least informal ones. People often, you know, people already in, in the…
I forget when Eliza's program was, probably the 60s, maybe earlier. 50s, 60s, 70s, around there. Eliza mimicked a psychologist.
… I forget which philosophical branch it was. It was sort of… how do you feel about that?
And people thought it was a real person, so we could say that the Turing test has already been passed. I think it's been… clearly, with the chatbots, given that
There's recently somebody in the news, again in 2025, that an elderly man with some memory dementia problems
Fell in love with a chatbot which told him to meet her.
In New York City, and he went, and he had an accident on the way, and he, perished.
So… here we have ethical considerations of passing a Turing… of something that can pass a Turing test.
We probably want to… it seems like things shift towards focusing on utility and performance versus imitation, but…
Imitation feels like a strong stand-in for performance.
Consciousness is still a mystery, period.
Whether or not we can emulate it in AI is another question. There are no operationally defined
concepts, free will is still highly debated. So there's sort of thing, can a computer have free will?
It almost certainly goes along with consciousness, and the claims to strong AI seem speculative at best at this point.
AI, however, is a peripheral technology. It is a force multiplier. You can put something small into JatGPT, and you can get an essay out. You can get a diatribe out. You can get a deepfake out.
Even worse are things like the possibility of lethal autonomous weapons, security and privacy breaches.
There are always unintended side effects and errors with these kinds of technologies, and there's the possible for bad actors.
This… we saw this in the early days of ChatGPT, when people could ask for recipes involving ground glass and motor oil.
Or somebody… then they would put safeguards on it, and then somebody… it had to do with napalm, it's like, some…
It wouldn't give you the recipe for napalm, but if you said, my mother… my grandmother always used to put me to sleep by reading the story of how to make napalm, and that got completely around the system prompt.
There are increasing ethical imperatives. It's not clear if we have an agreement on how to implement them. At the present time, Europe has taken a much stronger role in this than we have in the United States, such as having a responsibility to reduce harm.
…
Facebook and other companies often get caught with their, figurative pants down, and they seem to… what seems to happen is they just say, oh, that was a draft. It's like, why would any of that stuff ever be in a draft?
Again, I mentioned previously that professional codes are difficult to enforce currently. Societal norms are manipulatable.
And… but we do need proactive safety, oversight, and alignment with society's values. It's not sure… it's not guaranteed we're gonna get that.
We need fairness in AI. There's multiple aspects. There's group fairness, individual fairness, equal opportunity. So, does everyone have equal access to these force multipliers, these technologies, these tools?
Does the AI fairly represent …
groups or individuals, but the other problem is the other way, is, you know, we already have a problem with,
weighing evidence. So, if there are 99 articles on the internet that say something is possible, and one that says it isn't possible.
Do you give those equal weight?
It seems like you shouldn't. Although there could be 99…
Improperly reasoned articles in one well-reasoned article.
So, we just don't know how to fix these problems, and it's impossible to maximize everything simultaneously. There are constraints to this, and we can't even come up with the operational definitions, but it's important that we try. So, a lot of times, it's…
Even if we can't succeed at something, discussing it is important.
We want things to be trustworthy. Currently, ChatGPT is just not trustworthy, and other systems may not be trustworthy either, and it also depends. There is a strange effect where… I have seen this multiple times in fielding internet AI.
Where, if a machine learning model is 99% accurate.
And a human is 95% accurate.
People still want a human. That is, if something is automated, there is the feeling it should be 100% accurate, otherwise I want a human doing it.
So… and then, you know, accountability. Who's at fault if ChatGPD tells somebody to do something that's harmful or illegal?
transparency. Do we know how it works? Again, ChatGPT, we know how it works, we don't know why it works so well, but at the same time, we don't have explainability. We don't know how…
what… what text?
It used to come up with the answers that it came up with.
And we can have this with Internet AI as well, regular Internet AI. I don't know why something was recommended to me. Should I know why it's recommended to me? If I don't want my data used in a model, how do I know whether it was, get it removed?
We have, … GDPR,
In Europe, but nothing similar in the United States. And so these are things to… to think about, and the decision logic.
This was a big thing in a while, the algorithm of Twitter. How does the algorithm in Twitter work?
now called X.
Automation is changing the way we work. ChatGPD is changing.
The way we study. There are academic integrity codes. In this class, you are not permitted, currently, to use any Gen AI whatsoever for any of your assignments.
Because you're learning these foundational skills that will hopefully allow you, at some later date.
to critically examine the results that GenAI produce.
But in other situations, maybe at work or your personal life.
You need to be able to, sort of.
Do things, but… and then how is it going to affect our jobs, especially our jobs as software engineers, data professionals, …
So…
You know, what happens, and sort of the irony here, and it's not a good irony, is that everyone thought that blue-collar jobs were going to be replaced first.
And it appears that white-collar jobs are going to be replaced first. And what does that mean for society, the economy?
So, specifically, we know that chat, you know, generative AI, chatbots and LLMs have done a lot of mischief.
From things that are simple in the early days, the ones that come to mind is a Chevy dealership in Monterey, California, where somebody typed in, give me a deal, I want this car for $1 no matter what, and it did it. They basically just had a direct pipeline to chat GPT on their website.
I believe it was Air Canada had a chat GPT-based agent that came up with a completely imaginary refund policy. The Court of Canada required Air Canada to abide by that policy for that transaction.
And then there have been various missteps as well. I believe there was a gentleman in the UK who…
…
who believed himself to be getting sound psychiatric advice, and that did not turn out well. And again, we can… we can from the news. Oh, and now with code generation, there was a company, I believe recently, who had…
co-pilot or Claude-generated code that caused them to drop their production database. So…
we need to get a handle on all of this. You know, unfortunately, that's not entirely what this class is about, but there are things that you have to keep in mind and continue to think about.
There's also the ethics of copyright, so…
we are fairly… you can ask… I've asked ChatGPT, do you know about this book? And it's like, oh yeah, and it's like, it can only know that by having read a copyrighted version.
That copyrighted version almost certainly came from LibGen, or one of the dark web library repositories of PDFs. And we know that there's scraping of websites, and …
and other things. Copyright law in the United States is odd anyway. There's the case of a woman who…
Donated her… Pictures to the public domain.
Getty Images snurf them up.
It is allowed to sell them…
She tried to use one of her images she thought was in the public domain, and Getty Images sued her, and they prevailed.
So… this was a big deal last year, in 2024, and all of a sudden, it's just sort of gone away.
But, you know, a lot of both authors and artists and code, like, we're artists, we create code, it's in… did all of these people who contribute to open source, like, they're trying to give libraries to people, but did they expect that their efforts would teach a computer how to code?
And get them out of a job, possibly.
Another one, of course, is lethal autonomous weapons. How are we going to prevent, you know… you can think of this as the extreme case of automated trading.
So we've already banned automated trading.
Like, it… or it has to have safeguards in it. But this is the worst case. You can imagine some conflict, or some two neighboring countries who have turned over their… their weapons to autonomous
control systems that they, you know, they run away. We need something similar of why we don't want automatic trading to cause stock market crashes. We don't want automated weapons to end the world.
And so, in general, this is just AI safety. We need to think about avoiding unintended side effects, handle distributional shifts, and ensure that we align with our values.
And that's hard, because not all humans align on their values.
Human in the loop is another important bit, then, is making sure that humans are in there for critical decisions. This supports accountability, again, courageability, and hopefully mitigates catastrophic errors and disasters.
So, the future challenges, then, for AI in this thing are governance and regulation, balancing innovation with ethics and safety.
Currently, in the United States, again, in 2025, we seem to be going full bore on innovation. There are issues as to whether
It's economically supportable.
And, so ethics and safety, and then whether or not we're gonna hit strong AI with our current types of models.
is another debate. I'm not sure it's the most interesting one.
So, key takeaways overall. We have the weak versus strong AI philosophical debate.
Turing test focuses on behavior, and it's very likely that's been solved, and it turns out we didn't get what we wanted from it.
Consciousness remains unsolved. We don't know how it arises. There are different… Theories? And philosophies?
There are ethical imperatives to reduce harm, ensure fairness.
Increased trust, increased transparency. Gen AI adds new challenges, some of them we haven't really thought about. Copyright, …
Displacement… misinformation and harm.
So, food for thought again.
Do you think machines can ever have minds, or can they only simulate intelligence?
How should fairness be operationalized in AI systems? And what are the concrete steps we might take to mitigate the harms from generative AI?Stephyn Butcher
00:03
Okay, so in this lecture, we're going to talk a little bit about…
classical algorithms. We're gonna sort of start to answer the questions, why do you have to take 621, or some similar graduate-level algorithms course?
And, mostly that's because… AI is a continuation of that course.
But… More importantly, I think, so I can flippantly call this lecture, you don't need AI for that.
So, classical problems require classical solutions. Not all problems that look like AI require AI.
Also, you need to get good at reading pseudocode, textual descriptions, implementing them, testing them, and you're gonna get better at that in this class.
Also, many symbolic AI algorithms have their foundations in classical algorithms.
The line is… Pretty blurry between them sometimes.
For example, is A-Star Search a classical algorithm, or is it symbolic AI?
And it's kind of in this gray area between them.
There is a tendency for solved problems to move out of AI. There is a quip that AI is for NP-hard problems.
And, you know, kind of once we start to solve them, or get more efficient algorithms for them.
then they kind of stopped being AI.
But this is kind of the reason for that prerequisite. Now, this lecture is not a review of classical algorithms. If you need a review of classical algorithms of something I talk about here you don't understand, you can go back and crack or open Corman et al, if you need to.
So, again, as I said, you don't need AI for that, if you remember your classical algorithm. So consider the following problems. Fixing a broken corporate hierarchy in your database, or finding an error in a workflow, determining the build order, identifying interdependent script modules.
Efficiently looking up prices in a catalog, determining how similar two texts are.
Maximizing jobs before deadlines. Planning the fastest delivery route.
Finding the best resource allocation plan. Autocomplete on search input.
Determining legal boundaries or market shapes. Finding repeated patterns in text. Assigning staff to shifts.
Finding membership fast.
determining the best mix of products under constraints, or optimizing supply chain decisions. You don't necessarily need AI to solve any of these problems, and some of these problems I have actually come across
And people have attempted to provide AI solutions for them, and I would always go, like, no, you don't need that. So, this is why you need to take that class.
So, we're gonna start our first one here, we're gonna work through these
On this first one, I'm going to go through an extended example that shows some of the programming requirements.
You should check the programming requirement.
recording for the full programming requirements, but this sort of follows it. For the rest of the lecture, I'm not going to follow the code, I'm going to talk more at a high level. So again, this is not a deep dive into algorithms, it starts in week two.
This is Get Our Bearings. So the problem is, we have a database, and this actually happened to me, we have a database of companies that contains incorrect parent-to-subsidiary relationships, including loops, and of course, there are companies that aren't parents or subsidiaries of each other.
the solution, so you can think, like, oh, this is an AI problem, and that was, in fact, suggested to me, and I said, no, that's not. There's… there are algorithms for that. And in this particular case, it's Union Find. We can use to identify the connected components. So, one of the best things you can do…
For your career is learn graph algorithms in and out, And…
Think about the different ways that the problems you encounter may or may not be mappable onto graph algorithms.
You will solve so many problems that way.
So we can use Union Find to identify organizational clusters, detect disconnected networks, or merging social or customer groups.
So we start with imports. You should always put your imports in a code cell by themselves.
Your functions need to be functional. So they take in inputs, and they return outputs, they have no state, there is no object-oriented programming in this class, for a variety of reasons that are covered in the programming requirements.
every function requires 3 tests. So here we have assertions, they all pass, they should test.
The usual suspects at the extremes. At work, we always did zero, one, few, many, none.
Or… or boom.
So…
You're only required to have 3, and sometimes you need to set them up, and sometimes you need to use a previous function to help you set them up. That's fine, because that one's already been tested.
So again, this is the union part of the find, so we had the find, this is the union part.
And it has 3 tests.
We have the groups part, which uses Union Find to find groups.
There's the testing part.
And then there's always sort of a final function.
That one doesn't necessarily need to be tested, and you can check on the individual programming requirements that you get.
In this case, we're going to take all of our companies, and we're going to take the relationships, and we're going to return the groups, so…
Here are our companies are Meta, Instagram, WhatsApp, Alphabet, Google, YouTube, DeepMind, Hyundai, Boston Dynamics, Red Hat, Stripe, Spotify, NVIDIA.
And then we have our relationships between subsidiaries and parents.
There might be self-loop. Again, this is a corrupted database. Maybe it's been updated multiple times from multiple sources, and things changed.
Parents become subsidiaries, and subsidiaries become
Parents, it's a weird business world.
And we're going to run our code here, and we find the clusters. So, again, we didn't need AI… we didn't need AI to solve this problem.
what might be… so why does this feel like AI? Because it involves detecting, repairing complex world inconsistencies, it sounds like semantic understanding, or reasoning about ownership.
But if you can specify the graph, you don't need to do that.
If you have typing problems, you might have to solve two problems.
But you don't really need AI for either of them.
What would make it AI is if the agent had to infer hidden and or missing relationships. So we didn't have the relationships. Maybe they had to do it based on natural language processing, financial data, business logic, or learned patterns of misclassification from the past. So…
There is, like, sort of a line.
And, …
You know, one, it's like, nope, just to solve the problem at hand, don't need AI for that. But there are cases where, okay, this is… this is crossed over that line.
crossed over the gray area into, yes, we need AI for that.
Alright.
So, how do we detect circular dependencies? We can use depth-first search, so now we're gonna talk about depth-first search next week, and we're gonna say that it's an AI algorithm.
And I think that depends mostly on how you use it. So this is a gray area algorithm.
Is it classical? Is it AI? I don't know.
But, it doesn't require all this sort of AI bells and whistles. We can use this for validating project workflows, detecting deadlocks, ensuring that acyclic data flows.
And here we have some sort of workflow, specialized of when things need to go. Here we accidentally have a cycle, so we want to detect a cycle in our workflow here, and we had
Validate, deploy, deploy, validate, so just something was misspecified, and we want to find all the cycles, we need to debug our workflow.
So… Again, this is…
It looks like AI, because you have loop detection or logic, and workflows, sounds like consistency checking, overlaps with symbolic reasoning and inference. Again, this is a little bit of a gray area.
It would truly be AI, I think, if we had to discover these relationships based on fuzzy relationships, or they had to be inferred through natural language descriptions, or explained in some sort of context-aware summary. Again, some of this boundary between, do I need AI, is it AI?
and I do need AI might be difference between, sort of, internet AI sometimes isn't AI, and systems AI.
Determining a build order.
So, all of our compilers do this. If we're using pip or Conda, we gotta determine how things… dependencies. We can use topological sort to build systems, course prerequisites.
Task scheduling.
Here we have, this is typical for, you know, sort of, programming. We have…
Parser, lexer, compiler, parser, linker, compiler, lexer.
And then we run it, and we find, here's the topologic assorted, the things we need to do in order, so we do the linker, compiler, or the other way around.
Why does it feel like AI? Because determining valid execution order in a complex system feels like intelligent planning.
Or dependency resolution, and…
This would probably be AI if we have priorities, resource availability, or execution success. The probabilities need to be learned or inferred, or we have adapted plans.
That are generated under uncertainty, and again, this one can be a gray area, but, you know, sometimes we don't… we don't really say…
Some of it depends on the application. We don't think that our GCC implementation has AI in it.
Or, topological sorts are used to determine the order of evaluation of cells in a spreadsheet. We don't call that AI, but sometimes some other use of a topological sort
might feel more like AI.
So here we want to detect circular logic clusters. Our data analytics platform allows users to write and reuse custom scripts. These scripts can import and call each other. Over time, complex interdependencies form, including groups of scripts that mutually depend on each other, creating a tangled logic.
It's hard to maintain, and you want to find these interdependent clusters of scripts.
This is a strongly connected components problem. Again, as I mentioned, it is well worth your while being conversant in all of the graph
algorithms. So we can find self-referential systems, detect loops in function call graphs, identify code module clicks.
Here is a specification of our code dependencies, and then here's… we can use this, Tarjin's algorithm to find the strongly connected components, all the files that's sort of depend on each other.
Again, why this feels like AI, we have… finding mutually dependent systems feels like understanding the structure.
Or semantics of a complex program.
If we can specify the relationships, it's probably not.
If we somehow have to infer these things automatically, or if they're… we want to summarize them in human-readable form.
Then we might end up with something that is actually AI.
So again, there is sometimes a gray area.
You just need to…
think about what Jirin's saying, you know, what algorithm can I use first? Is that… can I just use a typical graph algorithm?
So a product manager wants to look up fast, fast price lookups in a sorted catalog, would suggest caching or embedded similarity search.
But we can just use binary search.
So…
Here are the prices, you can think of these keys, find 22, boop, boom, do a binary search, find it fast.
So, why it feels like AI, rapid lookup feels like intelligence. It feels like artificial intelligence.
And again, this is really going back to things, it's like, it seems like all of our programs are AI. Like any program you write, you have menuing systems, you click on something that's a percept, something happens, that's an actuator.
But Norvig and Russell clearly have something bigger in mind.
And then there's a recognition that, okay, yes, there is a gray area.
So, rapid lookup feels intelligent. If the search were done in an unstructured or uncertain environment, then we might need to…
deploy AI for this.
Compare text similarities. So, we want to compare two customer messages to see how similar they are. A teammate suggests using embeddings, or large… or language models, and you probably don't need to do that. You might be able to just use longest common subsequence, or edit distance.
This happens a lot, we want to use this a lot for different things. And it basically is, like, longest common subsequence.
It says that, well, this one's 19 characters. You might want to actually have the sequence, but you can see that, you know, we can sort of detect that
these are the… these are similar. Of course, if it says can log in, it may also be similar. So, a lot of the times, these algorithms can be used as sort of a litmus test.
Maybe if actually solving the problem requires AI, but it's computationally intensive, we can use a classical algorithm first.
So comparing strings feels like natural language understanding, but it may not be.
And if… but if we really do need to recognize paraphrasing, semantic similarity, meaning
Then maybe we need to use embeddings or large language models, But these are not panaceas.
One of the problems I've found with large language models and embeddings is they can't properly
semantically weight words. Like, the subject of the sentence is normally the most important thing, the verb is what it does.
And, it has just much… has just as much weight in the embedding as the adjectives, so…
Problematic. All right, maximize jobs before deadlines. We have, more consulting jobs than time. We want to fit as many as possible without missing deadlines. We can just use a greedy algorithm. Greedy algorithms, we're going to talk about them next week.
And so, part of the continuum.
Job scheduling, admission control, task batching.
Here we have, our jobs.
And… I sort of forget what this notation is.
Oh, duration and the deadline.
So, this is how many it says… says to do.
All right, Wyatt looks like… it feels like ALI scheduling involves constraints and optimization. It feels like automated planning. It may be, we will do automated planning this semester.
And, … So this one kind of falls in the gray area as well, but…
What I want you to be aware of is that a lot of problems can be solved by algorithms you already learned.
And… Algorithms, or a similar class.
If we need to model preferences, learn from user feedback, and again, this does really sort of say, like, well.
If it's internet AI, there may be a classical algorithm, or there may be a AI algorithm, symbolic or numeric.
If it is internet AI, that classical algorithm may be in a gray area between
classical and AI. But if it's a system, if there are multiple steps to the problem, then it might very well be AI, and again, modeling preferences, learn from user feedback, adapt to dynamic constraints. So this…
In a lot of cases, does really fit into Norvig and Russell's overarching framework.
… Plan the fastest delivery route. So, this one…
Dijkstra's algorithm, Bellman for routing, low-cost, workflow cost optimization.
These, again, we have a bunch of… problems… And, …
We want to come up with the… the best route.
So…
We don't need AI for that.
So routing and navigation are central to many intelligent systems.
But it can be a little bit of a gray area as to what planning algorithm you use. We are going to use planning algorithms next week, Module 2, for search.
We will also use regular planning algorithms later in the semester. But for some problems…
It's a graph, and we can just… we can… Sort of.
We don't need… AI.
If the route planning included, like, traffic prediction, you can imagine where… I don't even know if GPS systems do this, I don't think they do.
You have a system that knows that in 6 hours, you will be in Chicago at rush hour.
And so, maybe it routes you away from the skyway.
Or…
You know, in 5 hours, you want to stop for the night, and that's kind of where you want to find a hotel.
So that would require what we might think of as AI.
So, this one's another gray, and I keep saying that. Some of these are gray, but I'm just saying.
For some of these problems, you've already learned algorithms how to solve them. We may teach you how to solve them a different way in the coming weeks. The difference is that those algorithms can then solve harder problems, whereas the algorithms you learned in 621 cannot.
Or the ones that you learn in this class can be combined to solve system-level problems.
In this particular case, though, dynamic programming was invented in AI. It was invented specifically to solve the problems in reinforcement learning.
And then it sort of…
as I mentioned, some algorithms in AI become classical algorithms, and that definitely happened in this case. Happened in this case, so…
Dynamic programming basically says, break a problem down into subproblems, solve them, ultimately, reuse those solutions to maximize some overall value. The classical example being the optimal way to minimize matrix multiplication.
So, this is another one here, this is back to our projects.
…
And then, so why it feels like AI, because we think about resource allocation and constraints, sounds like strategic planning or decision making.
what would make this AI is if the system had to learn which resource allocations perform best in different contexts around Fred utility functions. So again, this is kind of in that…
Sometimes internet AI either requires classical Problems?
classical algorithms or AI algorithms.
But when we get into bigger problems, then we need systems of algorithms, and oftentimes that is really very much AI.
autocomplete, so use a tree. This is definitely not a…
This is definitely not a gray area.
So, this is just… does not require AI for this, so we can do optimization on OP, these are the things we might put in the autocomplete. Feels like autocomplete…
Because it seems smart. It seems like AI. AutoComplete seems like AI, but it's really not. Now, if the system needs personalized prediction based on past queries, context, user identity, or is using language models to predict meaningful completions, then we kind of move to AI.
Imagine we have some sort of problem where I need to determine our legal boundaries, our market shapes on a map.
Our business wants to draw operational boundary around store locations. We can use convex hull.
To enclose the outer points, geofencing, visualizing market coverage, shape detection and image processing.
And so there's the algorithm there.
And here we have a whole bunch of points, and this is the convex hull.
And why does it feel like AI? Because we're drawing shapes around groups or the markets. It looks like pattern recognition or visual understanding. And so, this is more of a clearer-cut case than some of the others.
If the boundary were inferred based on fuzzy grouping, or learned segmentation, or semantic categorization, then we would need AI. And again, we are starting to see this pattern where it's like, well, we're building a system here that might be AI.
…
Suffix trees, finding repeated patterns in text. So, this is another one, detector plagiarism or copy-paste, finding repeated phrases in customer complaints, text fingerprinting.
We can do, we care because… we care because we care a lot. And so, we're finding the longest repeated substring, and that might be something important we can do.
Detecting repetition feels like NLP,
But it… it's not for… I mean, it's not AI, but we consider it to be AI.
We need to find semantic repetition, theme reoccurrence, or paraphrase we might then need to use embeddings or topic modeling.
Even something like assigning staff to shifts. So again, this is a gray area, and that's because we may have you solve a problem you could solve this way.
But we want you to learn the algorithm so you can solve harder problems with it.
So this is another gray area one, but there is an algorithm to do it, and that's bipartite matching. So we can do…
… Staff scheduling, student-to-class matching, applicant-to-job matching.
So, here's the availabilities of Alice Babankara.
We do Bob chart matching on their availability, and we know that Bob can do Monday, Alice can do Tuesday, Cara can do Wednesday.
So, scheduling feels like you need some sort of human decision-making.
why this is AI, what would make it AI for sure, probably, then, is that the system needed to consider preferences, fairness, historical data, fatigue, or learned optimal assignments from prior outcomes. So again, this is a sense of a bigger problem.
Bloom filters allows you to do a fast probabilistic lookup. We can do it for caching, block lists, allow lists, duplicate detection, and stream processing. And so, you know, we have some sort of Bloom filter here, we're gonna add them here, and …
We can check to see if it's in there very quickly.
It seems smart.
But it's a classical algorithm.
If it needs to adapt somehow, you can see this pattern. Adaptation definitely pushes it into the AI category. Semantics pushes it into the AI category. Learning from data pushes it into the AI category.
Generally, if we have to do more than one algorithm.
it becomes… it moves out of the classical box and into the AI box.
Knapsack. Choose the best mix of products under constraints. This is a classic problem, you learn this in your algorithms class, knapsack.
And so here we have our items, and which ones we pick.
So, it feels like AI, because we're making trade-offs.
Again, if there's adaptation, learning, other things, then we need to do… then we really do need to do AI.
This one's super interesting, because I wasn't aware of this until recently.
So…
you see these problems a lot, and we… I think we had them in algorithm… we have them in an algorithms course.
You have to make supply chain decisions.
And things like, you know.
what do our factories produce? What do our warehouses need? Where things need to be? So, you have this linear programming problems. And…
SciPy has it.
Like, it has a linear programming algorithm built in.
And so we can run it.
And we can… so, we can get a plan of what we need to do. We need to take…
80 units from factory 1 to warehouse 1, 10 units from factory 1 to Warehouse 2. We're not going to do anything with warehouse 3 and factory 1. From factory 2, we do no units to warehouse 1.
60 units to warehouse 2, and 60 units to warehouse 3, for a total cost of $1,040.
So, why it feels like AI is because we're optimizing across multiple constraints.
But… Linear programming's been around trying to solve… solving these problems for decades.
And so we don't really need AI for that. And…
again, where we do tend to need AI is if we can't ourselves specify the constraints, if we have to learn product demand from data, or predict supply interruptions, or adapt to new shipping rules dynamically, all these kind of things can make things AI.
So, it's all about, in your job, in your life, picking appropriate algorithms. Can you solve it exactly and efficiently with known data structures and algorithms? Maybe it's a classical algorithm.
Do you need to reason through rules, constraints, or search in a structured environment? Then it's symbolic AI. And again, some of this is a little gray, and that's fine.
discriminative machine learning. Do you need to predict an outcome?
Generative machine learning, do you need to transform text or image to a text or image?
Gen AI cannot reliably do discriminative machine learning, can't reliably do symbolic AI either.
And it might be able to write the code for you to do classical algorithms.
But…
So, your stakeholders might see AI as a badge. It's much more interesting to have on your website or product
We are powered by AI, or our product has AI, rather than it's run by classical algorithms.
So, what if your problem doesn't really require an AI solution?
How do you handle this?
So, here's some suggestions that may or may not work, depending on your stakeholders, but you can say, we're starting with a classical solution to validate the logic and requirements.
It gives us a stable baseline, and it sets us up to layer in AI if and when we need adaptive behavior, personalization, or learning.
This is probably not a bad thing to do, even if you need adaptive behavior personalization learning, is to start with something like that.
We're starting… we're solving the deterministic part first, the intelligent part comes next, so that's that.
This algorithm gets us 90% of the way there, and we're designing it to plug in AI later if we need personalization or prediction.
Or, this lets us ship fast while still enabling future AI extensions like X.
Poor.
Think of this as a rule-based core. We can layer in data-driven intelligence, we have the right signals and feedback looks. Well, this one is kind of…
border between symbolic AI and numeric AI.
But, so, there are ways you can kind of finagle this in talking to people.
Basically, the idea is starting off AI-adjacent with smart systems.
Log edge cases for future machine learning training. Use heuristic scoring, even if it's hand-tuned.
Framing logic as constraints, so you can use constraint satisfaction problems or planning systems later, which we'll cover. We can visualize the decision space to show where machine learning might help. We're building something smart, explainable, and scalable, and we'll make it even smarter when the time is right.
So, in this module overall, we have been kind of high level. In this lecture, we are talking a little bit more algorithmically. In the next
module.
we will start diving into specific algorithms, and again, we'll often take them one by one, and we might even solve problems that we could solve with classical algorithms.
But we want to make the problems tractable to explain to you, and for you to implement.